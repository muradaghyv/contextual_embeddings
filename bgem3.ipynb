{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28d5afe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from FlagEmbedding import BGEM3FlagModel\n",
    "import os\n",
    "\n",
    "os.environ['CURL_CA_BUNDLE'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d2bfb83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f95275f8273b4ae6b904bc05c4ef3b05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 30 files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = BGEM3FlagModel(\"BAAI/bge-m3\",\n",
    "                       use_fp16=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5ec0349",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_1 = [\"Python nədir?\", \"Böyük dil modelləri nədir?\"]\n",
    "sentences_2 = [\"Mənim adım Muraddır və Maşın Öyrənməsi mühəndisiyəm.\", \n",
    "               \"Böyük dil modeli insan dilini yarada bilən və əlaqəli vəzifələri yerinə yetirə bilən bir süni intellekt növüdür..\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79911200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.37017542 0.39612377]\n",
      " [0.30685663 0.6944343 ]]\n"
     ]
    }
   ],
   "source": [
    "embeddings_1 = model.encode(sentences_1, \n",
    "                            batch_size=12, \n",
    "                            max_length=8192, # If you don't need such a long length, you can set a smaller value to speed up the encoding process.\n",
    "                            )['dense_vecs']\n",
    "embeddings_2 = model.encode(sentences_2)['dense_vecs']\n",
    "similarity = embeddings_1 @ embeddings_2.T\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0545c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = model.tokenizer(text=sentences_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dde5a898",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁Mənim', '▁adım', '▁Murad', 'dır', '▁və', '▁Ma', 'şın', '▁Ö', 'yr', 'ən', 'məsi', '▁mühəndis', 'iy', 'əm', '.']\n",
      "[0, 177565, 54854, 148674, 2544, 530, 911, 83199, 5973, 12271, 7879, 28932, 234142, 12712, 10857, 5, 2]\n",
      "Token: ▁Mənim, Original text: ''\n",
      "Token: ▁adım, Original text: 'Mənim'\n",
      "Token: ▁Murad, Original text: ' adım'\n",
      "Token: dır, Original text: ' Murad'\n",
      "Token: ▁və, Original text: 'dır'\n",
      "Token: ▁Ma, Original text: ' və'\n",
      "Token: şın, Original text: ' Ma'\n",
      "Token: ▁Ö, Original text: 'şın'\n",
      "Token: yr, Original text: ' Ö'\n",
      "Token: ən, Original text: 'yr'\n",
      "Token: məsi, Original text: 'ən'\n",
      "Token: ▁mühəndis, Original text: 'məsi'\n",
      "Token: iy, Original text: ' mühəndis'\n",
      "Token: əm, Original text: 'iy'\n",
      "Token: ., Original text: 'əm'\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the BGE-M3 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"BAAI/bge-m3\")\n",
    "\n",
    "# Text to tokenize\n",
    "text = \"Mənim adım Muraddır və Maşın Öyrənməsi mühəndisiyəm.\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "# Print the tokens\n",
    "print(tokens)\n",
    "\n",
    "# If you want to see the token IDs\n",
    "token_ids = tokenizer.encode(text)\n",
    "print(token_ids)\n",
    "\n",
    "# To see the exact boundaries in the original text, you can use\n",
    "encoded = tokenizer(text, return_offsets_mapping=True)\n",
    "for token, (start, end) in zip(tokens, encoded[\"offset_mapping\"]):\n",
    "    print(f\"Token: {token}, Original text: '{text[start:end]}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7fbebbca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['▁', 'ASAN', '▁xidmət', '▁mərkəzləri', 'nə', '▁xoş', '▁gəl', 'mi', 'siniz', '!']\n",
      "Token IDs: [6, 95239, 25058, 212523, 5022, 129395, 66366, 266, 25098, 38]\n",
      "Token: ASAN, Original text: 'A'\n",
      "Token: ▁xidmət, Original text: 'ASAN'\n",
      "Token: ▁mərkəzləri, Original text: ' xidmət'\n",
      "Token: nə, Original text: ' mərkəzləri'\n",
      "Token: ▁xoş, Original text: 'nə'\n",
      "Token: ▁gəl, Original text: ' xoş'\n",
      "Token: mi, Original text: ' gəl'\n",
      "Token: siniz, Original text: 'mi'\n",
      "Token: !, Original text: 'siniz'\n"
     ]
    }
   ],
   "source": [
    "# Access the tokenizer from the model\n",
    "tokenizer = model.tokenizer\n",
    "\n",
    "# Text to tokenize\n",
    "text = \"ASAN xidmət mərkəzlərinə xoş gəlmisiniz!\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "# Get token IDs\n",
    "token_ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "print(\"Token IDs:\", token_ids)\n",
    "\n",
    "# If you want to see the mapping between tokens and original text\n",
    "# We need to get the offsets manually as FlagEmbedding might not have direct support\n",
    "# for offset_mapping like transformers\n",
    "encoded_plus = tokenizer.encode_plus(text, return_offsets_mapping=True)\n",
    "if \"offset_mapping\" in encoded_plus:\n",
    "    # If offset_mapping is supported\n",
    "    for token, (start, end) in zip(tokens, encoded_plus[\"offset_mapping\"]):\n",
    "        if start != end:  # Skip special tokens which might have (0,0) offsets\n",
    "            print(f\"Token: {token}, Original text: '{text[start:end]}'\")\n",
    "else:\n",
    "    # Alternative approach if offset_mapping isn't directly supported\n",
    "    print(\"Note: Direct offset mapping not available in this implementation.\")\n",
    "    # Display tokens with their positions in the sequence\n",
    "    for i, token in enumerate(tokens):\n",
    "        print(f\"Position {i}: Token: {token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fff9843",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
