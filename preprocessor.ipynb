{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pypdf\n",
    "import re\n",
    "from typing import List\n",
    "import os\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "import os\n",
    "\n",
    "os.environ['CURL_CA_BUNDLE'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = pypdf.PdfReader(\"ai_doc.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = \"\"\n",
    "for i in range (0, 8):\n",
    "    text = reader.pages[i].extract_text()\n",
    "    all_text += text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Clean and normalize text content\"\"\"\n",
    "    # Removing multiple lines\n",
    "    text = re.sub(r\"\\n+\", \"\\n\", text)\n",
    "\n",
    "    # Removing multiple spaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "    # Remove HTML comments \n",
    "    text = re.sub(r\"<!--.?-->\", \"\", text)\n",
    "    text = re.sub(r\"&\", \"u\", text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_chunks(text: str, max_chunk_size: int = 1024) -> List[str]:\n",
    "    \"\"\"Creates chunks with size of max_chunk_size parameter from the document.\"\"\"\n",
    "    chunks = []\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text) # Splitting sentences from the document\n",
    "    current_chunk = []\n",
    "    current_size = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence_size = len(sentence)\n",
    "        if current_size+sentence_size > max_chunk_size and current_chunk:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = []\n",
    "            current_size = 0\n",
    "        \n",
    "        current_chunk.append(sentence)\n",
    "        current_size += sentence_size\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = clean_text(text=all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = split_into_chunks(text=clean, max_chunk_size=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, chunk in enumerate(chunks):\n",
    "    filename = f\"chunk_{i}.txt\"\n",
    "    filepath = f\"/home/murad/Documents/self-study/contextual_embeddings/chunks/{filename}\"\n",
    "\n",
    "    try:\n",
    "        with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(chunk)\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving chunk to {filepath}: {str(e)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents(path = \"/home/murad/Documents/self-study/contextual_embeddings/chunks\"):\n",
    "    \"\"\"Loading all processed documents.\"\"\"\n",
    "    # Loading Python docs\n",
    "    documents = []\n",
    "    for file_path in os.listdir(path):\n",
    "        with open(os.path.join(path, filepath), \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "        documents.append(content)\n",
    "    \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/murad/Documents/self-study/contextual_embeddings/chunks\"\n",
    "docs = []\n",
    "for file_path in os.listdir(path):\n",
    "    with open(os.path.join(path, file_path), \"r\", encoding=\"utf-8\") as f:\n",
    "        content = f.read()\n",
    "    docs.append(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fe51684ae29458183d569d1efb75b1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 30 files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Then load your model\n",
    "model = BGEM3FlagModel(\"BAAI/bge-m3\", \n",
    "                      use_fp16=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [clean_text(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "embeddings_1 = model.encode(docs, \n",
    "                            batch_size=12, \n",
    "                            max_length=8192, # If you don't need such a long length, you can set a smaller value to speed up the encoding process.\n",
    "                            )['dense_vecs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings_1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using ChromaDB directory: /tmp/tmpf9dwtvil\n",
      "Successfully stored 37 documents with embeddings in ChromaDB\n",
      "Collection name: pdf_chunks\n",
      "Database location: /tmp/tmpf9dwtvil\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "import tempfile\n",
    "import numpy as np\n",
    "\n",
    "# Create a temporary directory with write permissions\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "print(f\"Using ChromaDB directory: {temp_dir}\")\n",
    "\n",
    "# Initialize ChromaDB client\n",
    "client = chromadb.PersistentClient(path=temp_dir)\n",
    "\n",
    "# Create a collection WITHOUT an embedding function\n",
    "collection = client.create_collection(\n",
    "    name=\"pdf_chunks\",\n",
    "    metadata={\"description\": \"PDF chunks with BGE-M3 embeddings\"}\n",
    ")\n",
    "\n",
    "# Prepare your data\n",
    "document_ids = [f\"chunk_{i}\" for i in range(len(docs))]\n",
    "metadatas = [{\"source\": f\"chunk_{i}.txt\"} for i in range(len(docs))]\n",
    "\n",
    "# Convert numpy arrays to lists if needed\n",
    "embeddings_list = []\n",
    "for emb in embeddings_1:\n",
    "    if isinstance(emb, np.ndarray):\n",
    "        embeddings_list.append(emb.tolist())\n",
    "    else:\n",
    "        embeddings_list.append(emb)\n",
    "\n",
    "# Add documents with pre-computed embeddings\n",
    "collection.add(\n",
    "    documents=docs,              # The document texts\n",
    "    ids=document_ids,            # Unique IDs\n",
    "    metadatas=metadatas,         # Metadata\n",
    "    embeddings=embeddings_list   # Your pre-computed embeddings\n",
    ")\n",
    "\n",
    "print(f\"Successfully stored {len(docs)} documents with embeddings in ChromaDB\")\n",
    "print(f\"Collection name: pdf_chunks\")\n",
    "print(f\"Database location: {temp_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = [\"Turinq testi nədir?\"]\n",
    "query_embedding = model.encode(sentences=query,\n",
    "                               batch_size=12,\n",
    "                               max_length=1024)[\"dense_vecs\"]\n",
    "query_embedding = query_embedding.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['chunk_16',\n",
       "   'chunk_3',\n",
       "   'chunk_22',\n",
       "   'chunk_29',\n",
       "   'chunk_25',\n",
       "   'chunk_20',\n",
       "   'chunk_7',\n",
       "   'chunk_21',\n",
       "   'chunk_23',\n",
       "   'chunk_11']],\n",
       " 'distances': [[0.6031309300372137,\n",
       "   0.7156820159450029,\n",
       "   0.9340197892554905,\n",
       "   1.069768826620262,\n",
       "   1.1289514818185007,\n",
       "   1.1640132529745169,\n",
       "   1.195128053477976,\n",
       "   1.2231285490888089,\n",
       "   1.2289387843506814,\n",
       "   1.2372925567881798]],\n",
       " 'metadatas': [[{'source': 'chunk_16.txt'},\n",
       "   {'source': 'chunk_3.txt'},\n",
       "   {'source': 'chunk_22.txt'},\n",
       "   {'source': 'chunk_29.txt'},\n",
       "   {'source': 'chunk_25.txt'},\n",
       "   {'source': 'chunk_20.txt'},\n",
       "   {'source': 'chunk_7.txt'},\n",
       "   {'source': 'chunk_21.txt'},\n",
       "   {'source': 'chunk_23.txt'},\n",
       "   {'source': 'chunk_11.txt'}]],\n",
       " 'embeddings': None,\n",
       " 'documents': [['Turinq, bir kompüterin süni zəkaya sahib ola biləcəyini və hakimi çaşdıracaq qədər inandırıcı ola biləcəyini iddia edir. Hakim, insanla yoxsa kompüterlə danışdığını anlamayacaq. Bu testə Turinq testi deyilir.Turinq testinin məqsədi və əhəmiyyəti Turinq testi, bir maşının insan səviyyəsində zəkaya sahib olub olmadığını ölçmək üçün bir meyardır. Testin məqsədi, bir maşının davranışının insan davranışına o qədər bənzəyib-bənzəmədiyini müəyyən etməkdir ki, onu ayırd etmək mümkün olmasın.',\n",
       "   'Turinq testi, süni zəka tədqiqatları sahəsində mühüm bir rol oynamış və bu sahədə bir çox müzakirəyə səbəb olmuşdur. Turinq testinin tənqidləri Turinq testi, süni zəkanın ölçülməsi üçün bir meyar olaraq tənqid edilmişdir. Bəzi tənqidçilər, testin yalnız bir maşının insan kimi davranma qabiliyyətini ölçdüyünü və həqiqi zəkanı ölçmədiyini iddia edir. Digər tənqidçilər isə testin subyektiv olduğunu və hakimin qərəzli ola biləcəyini iddia edir. Turinq testinin təsiri Turinq testi, süni zəka tədqiqatları sahəsində böyük təsirə malik olmuşdur.',\n",
       "   'Bunlardan bir neçəsi aşağıda qeyd olunmuşdur: T\\x03rinqtesti vəint\\x03itivyanaşma Modern kompüterlərin atası hesab edilən Alan Turinq, 1950-ci ildə \"Mind\" jurnalında nəşr olunan \"Hesablama Maşınları və Zəka\" adlı məqaləsində süni zəka ilə bağlı ilk düşüncələrini ortaya qoymuşdur. Bu məqalədə o, insan və kompüter arasında aparılan bir testdən bəhs edir. Bu testdə bir insan hakim rolunu oynayır və bir insanla bir kompüterlə sual-cavab aparır. Hakim, nə insanı, nə də kompüteri görə bilir.',\n",
       "   'Kompüter \"şüurluluğunun\" da hər hansı bir həddi yoxdur, lakin süni zəkanın yaradılmasına bir sıra hipofizlər verilib, bunlara da Turinq testini və Nyuel-Saymon hipofizini misal göstərmək olar. Ona görə də süni zəka probleminin qoyulmasına iki cür yaxınlaşmaq olar: Yuxarıdan aşağıya, semiotik – ekspert sistemlər, biliklər və məntiqi qərarlar bazası, yüksəksəviyyəli psixi prosesləri təqlid edən sistemlərin yaradılması, məsələn, düşüncə, danışıq, incəsənət və s.',\n",
       "   'Baxmayaraq ki, Nyuell-Saymon fərziyyəsi, yaxud Turinq testi kimi, bir sıra fərziyyələr irəli sürülmüşdü, kompüterin \"ağlabatan\" səvviyyəsinə qədər inkişaf etməsinin dəqiq sübutu yoxdur. Robotexnika Robotexnika sahəsi və süni zəka elmi bir-biri ilə sıx əlaqədardır. Robotexnikada süni zəkadan istifadə edib intellektual robotların yaradılması süni zəka elminin yeni bir sahəsini yaradır. İntellektuallıq üzərində cisimləri manipulyasıya etmək, olduğu yeri tapmaq, ətraf aləmi təhlil etmək və hərəkəti planlaşdırmaq üçün işləmək lazımdır.',\n",
       "   '· İllik RoboCup turnirində, robotların, futbolun sadələşdirilmiş formasında rəqabətin aparılması. · Virtual söhbət proqramları. ChatMaster- Virtual söhbət proqramları insan danışığını təqlid edən, bir və daha çox insanla söhbət edən kompüter proqramıdır. İlk belə proqramlardan biri 1969-cu ildə Cozef Beyzenbaum tərəfindən yaradılan Eliza proqramı olmuşdur. Belə düşünülür ki, ideal virtual söhbət proqramı Turinq testini keçə biləcək. Chatmaster özü-özünü öyrədə bilən, insanla dialoq apara bilən proqramdır. Dialoq klaviatura ilə aparılır və ekranda əks etdirilir.',\n",
       "   'Test, bu sahədə bir çox tədqiqat və inkişafa ilham vermiş və süni zəkanın nə olduğu və necə ölçülməsi lazım olduğu barədə müzakirələrə səbəb olmuşdur. Simvolikyanaşma Süni zəka probleminə simvolik yanaşma ilk rəqəmsal maşınların yaranması dövründə yaranıb. Bu yanaşmada problemlərin həllinin tapılmasının uğur və səmərəliyi üçün informasiyanın abstraktlığı vacibdir. Məntiqiyanaşma Süni zəka sisteminin yaradılmasına məntiqi yanaşma modelləşdirilmiş mühakiməyə əsaslanır. Teorik əsas kimi məntiqə əsaslanır.',\n",
       "   'Müəllimsiz öyrənmə cisimləri olduğu kimi qavramağa icazə verir. Müəllim ilə öyrənmə isə sinifləndirməyə və reqressiv analizə kömək edir. Sinifləndirmə onun üçün istifadə olunur ki, cismin hansı kateqoriyaya aid olduğu aydınlaşdırılsın. Reqressiv analiz isə ədədi sıralarda giriş ədədlərindən istifadə olunan funksiya tapılması ilə çıxış ədədinin proqnozlaşması üçün istifadə olunur. Təlim prosesi zamanı maşının düzgün cavabları mükafatlandırılır, yanlış cavabları isə cəzalandırılır. Müasir dövrdəki süni zəkanın əsası 1956-cı ildəDartmut Kollecində keçirilən konferensiyada qoyulmuşdur.',\n",
       "   'Ars Maqnaya istifadəçi xristianlıq haqqında öz sualını daxil edir və aparat insanın köməyi olmadan dərhal avtomatik olaraq bu suala cavab tapırdı. Bu aparat müsəlmanları xristianlaşdırmaq məqsədilə yaradılmışdır. Maşınların təlimi sərbəst şəkildə süni zəka biliklərinin və onun işləmə proseslərinin alınmasına aiddir. Bu süni zəka elminin ilk dövrlərindən mərkəzi olmuşdur. 1956-cı ildə Darmund yaz konferansında Rey Solomonoff öz-özünü öyrədə bilən maşının ola biləcəyi haqqında hesabat yazdı və onu \"İntuitiv nəticə maşını\" adlandırdı. Maşın təliminin iki növü vardır; müəllimsiz və müəllim ilə.',\n",
       "   'AI) — insanların və ya heyvanların intellektindən fərqli olaraq maşın və ya proqram təminatının intellekti.[1] Süni intellekt tətbiqlərinə qabaqcılveb axtarış mühərrikləri (Quql axtarış sistemi), tövsiyə sistemləri (\"YouTube\", \"Amazon\" və \"Netflix\" tərəfindən istifadə olunur),virtual assistentlər (\"Siri\" və \"Alexa\" kimi), özügedən avtomobillər (\"Waymo\"), generativ və ya yaradıcı alətlər (\"ChatGPT\" və süni intellekt incəsənəti), strateji oyunlarda (şahmat və Qo kimi) ən yüksək səviyyədə yarışmaq daxildir.[2] Süni intellekt 1956-cı ildə akademik fənn kimi təsis edilmişdir.] Bu sahə optimizm dövrlərindən keçmiş, sonradan məyusluq və maliyyə itkisi yaşamış, lakin 2012-ci ildən sonra dərin öyrənmə bütün əvvəlki süni intellekt üsullarını üstələmiş,[8] beləliklə bu sahədə maliyyələşmə və maraqda böyük artım müşahidə edilmişdir.']],\n",
       " 'uris': None,\n",
       " 'data': None}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection.query(query_embeddings=query_embedding,\n",
    "                 n_results=10,\n",
    "                 include=[\"documents\", \"distances\", \"metadatas\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
