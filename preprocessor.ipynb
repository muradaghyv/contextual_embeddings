{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pypdf\n",
    "import re\n",
    "from typing import List\n",
    "import os\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "import os\n",
    "\n",
    "os.environ['CURL_CA_BUNDLE'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = pypdf.PdfReader(\"ai_doc.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = \"\"\n",
    "for i in range (0, 8):\n",
    "    text = reader.pages[i].extract_text()\n",
    "    all_text += text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Clean and normalize text content\"\"\"\n",
    "    # Removing multiple lines\n",
    "    text = re.sub(r\"\\n+\", \"\\n\", text)\n",
    "\n",
    "    # Removing multiple spaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "    # Remove HTML comments \n",
    "    text = re.sub(r\"<!--.?-->\", \"\", text)\n",
    "    text = re.sub(r\"&\", \"u\", text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_chunks(text: str, max_chunk_size: int = 1024) -> List[str]:\n",
    "    \"\"\"Creates chunks with size of max_chunk_size parameter from the document.\"\"\"\n",
    "    chunks = []\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text) # Splitting sentences from the document\n",
    "    current_chunk = []\n",
    "    current_size = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence_size = len(sentence)\n",
    "        if current_size+sentence_size > max_chunk_size and current_chunk:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = []\n",
    "            current_size = 0\n",
    "        \n",
    "        current_chunk.append(sentence)\n",
    "        current_size += sentence_size\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = clean_text(text=all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = split_into_chunks(text=clean, max_chunk_size=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, chunk in enumerate(chunks):\n",
    "    filename = f\"chunk_{i}.txt\"\n",
    "    filepath = f\"/home/murad/Documents/self-study/contextual_embeddings/chunks/{filename}\"\n",
    "\n",
    "    try:\n",
    "        with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(chunk)\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving chunk to {filepath}: {str(e)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents(path = \"/home/murad/Documents/self-study/contextual_embeddings/chunks\"):\n",
    "    \"\"\"Loading all processed documents.\"\"\"\n",
    "    # Loading Python docs\n",
    "    documents = []\n",
    "    for file_path in os.listdir(path):\n",
    "        with open(os.path.join(path, filepath), \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "        documents.append(content)\n",
    "    \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/murad/Documents/self-study/contextual_embeddings/chunks\"\n",
    "docs = []\n",
    "for file_path in os.listdir(path):\n",
    "    with open(os.path.join(path, file_path), \"r\", encoding=\"utf-8\") as f:\n",
    "        content = f.read()\n",
    "    docs.append(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fe51684ae29458183d569d1efb75b1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 30 files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Then load your model\n",
    "model = BGEM3FlagModel(\"BAAI/bge-m3\", \n",
    "                      use_fp16=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [clean_text(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "embeddings_1 = model.encode(docs, \n",
    "                            batch_size=12, \n",
    "                            max_length=8192, # If you don't need such a long length, you can set a smaller value to speed up the encoding process.\n",
    "                            )['dense_vecs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings_1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using ChromaDB directory: /tmp/tmpf9dwtvil\n",
      "Successfully stored 37 documents with embeddings in ChromaDB\n",
      "Collection name: pdf_chunks\n",
      "Database location: /tmp/tmpf9dwtvil\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "import tempfile\n",
    "import numpy as np\n",
    "\n",
    "# Create a temporary directory with write permissions\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "print(f\"Using ChromaDB directory: {temp_dir}\")\n",
    "\n",
    "# Initialize ChromaDB client\n",
    "client = chromadb.PersistentClient(path=temp_dir)\n",
    "\n",
    "# Create a collection WITHOUT an embedding function\n",
    "collection = client.create_collection(\n",
    "    name=\"pdf_chunks\",\n",
    "    metadata={\"description\": \"PDF chunks with BGE-M3 embeddings\"}\n",
    ")\n",
    "\n",
    "# Prepare your data\n",
    "document_ids = [f\"chunk_{i}\" for i in range(len(docs))]\n",
    "metadatas = [{\"source\": f\"chunk_{i}.txt\"} for i in range(len(docs))]\n",
    "\n",
    "# Convert numpy arrays to lists if needed\n",
    "embeddings_list = []\n",
    "for emb in embeddings_1:\n",
    "    if isinstance(emb, np.ndarray):\n",
    "        embeddings_list.append(emb.tolist())\n",
    "    else:\n",
    "        embeddings_list.append(emb)\n",
    "\n",
    "# Add documents with pre-computed embeddings\n",
    "collection.add(\n",
    "    documents=docs,              # The document texts\n",
    "    ids=document_ids,            # Unique IDs\n",
    "    metadatas=metadatas,         # Metadata\n",
    "    embeddings=embeddings_list   # Your pre-computed embeddings\n",
    ")\n",
    "\n",
    "print(f\"Successfully stored {len(docs)} documents with embeddings in ChromaDB\")\n",
    "print(f\"Collection name: pdf_chunks\")\n",
    "print(f\"Database location: {temp_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = [\"Turinq testi nÉ™dir?\"]\n",
    "query_embedding = model.encode(sentences=query,\n",
    "                               batch_size=12,\n",
    "                               max_length=1024)[\"dense_vecs\"]\n",
    "query_embedding = query_embedding.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['chunk_16',\n",
       "   'chunk_3',\n",
       "   'chunk_22',\n",
       "   'chunk_29',\n",
       "   'chunk_25',\n",
       "   'chunk_20',\n",
       "   'chunk_7',\n",
       "   'chunk_21',\n",
       "   'chunk_23',\n",
       "   'chunk_11']],\n",
       " 'distances': [[0.6031309300372137,\n",
       "   0.7156820159450029,\n",
       "   0.9340197892554905,\n",
       "   1.069768826620262,\n",
       "   1.1289514818185007,\n",
       "   1.1640132529745169,\n",
       "   1.195128053477976,\n",
       "   1.2231285490888089,\n",
       "   1.2289387843506814,\n",
       "   1.2372925567881798]],\n",
       " 'metadatas': [[{'source': 'chunk_16.txt'},\n",
       "   {'source': 'chunk_3.txt'},\n",
       "   {'source': 'chunk_22.txt'},\n",
       "   {'source': 'chunk_29.txt'},\n",
       "   {'source': 'chunk_25.txt'},\n",
       "   {'source': 'chunk_20.txt'},\n",
       "   {'source': 'chunk_7.txt'},\n",
       "   {'source': 'chunk_21.txt'},\n",
       "   {'source': 'chunk_23.txt'},\n",
       "   {'source': 'chunk_11.txt'}]],\n",
       " 'embeddings': None,\n",
       " 'documents': [['Turinq, bir kompÃ¼terin sÃ¼ni zÉ™kaya sahib ola bilÉ™cÉ™yini vÉ™ hakimi Ã§aÅŸdÄ±racaq qÉ™dÉ™r inandÄ±rÄ±cÄ± ola bilÉ™cÉ™yini iddia edir. Hakim, insanla yoxsa kompÃ¼terlÉ™ danÄ±ÅŸdÄ±ÄŸÄ±nÄ± anlamayacaq. Bu testÉ™ Turinq testi deyilir.Turinq testinin mÉ™qsÉ™di vÉ™ É™hÉ™miyyÉ™ti Turinq testi, bir maÅŸÄ±nÄ±n insan sÉ™viyyÉ™sindÉ™ zÉ™kaya sahib olub olmadÄ±ÄŸÄ±nÄ± Ã¶lÃ§mÉ™k Ã¼Ã§Ã¼n bir meyardÄ±r. Testin mÉ™qsÉ™di, bir maÅŸÄ±nÄ±n davranÄ±ÅŸÄ±nÄ±n insan davranÄ±ÅŸÄ±na o qÉ™dÉ™r bÉ™nzÉ™yib-bÉ™nzÉ™mÉ™diyini mÃ¼É™yyÉ™n etmÉ™kdir ki, onu ayÄ±rd etmÉ™k mÃ¼mkÃ¼n olmasÄ±n.',\n",
       "   'Turinq testi, sÃ¼ni zÉ™ka tÉ™dqiqatlarÄ± sahÉ™sindÉ™ mÃ¼hÃ¼m bir rol oynamÄ±ÅŸ vÉ™ bu sahÉ™dÉ™ bir Ã§ox mÃ¼zakirÉ™yÉ™ sÉ™bÉ™b olmuÅŸdur. Turinq testinin tÉ™nqidlÉ™ri Turinq testi, sÃ¼ni zÉ™kanÄ±n Ã¶lÃ§Ã¼lmÉ™si Ã¼Ã§Ã¼n bir meyar olaraq tÉ™nqid edilmiÅŸdir. BÉ™zi tÉ™nqidÃ§ilÉ™r, testin yalnÄ±z bir maÅŸÄ±nÄ±n insan kimi davranma qabiliyyÉ™tini Ã¶lÃ§dÃ¼yÃ¼nÃ¼ vÉ™ hÉ™qiqi zÉ™kanÄ± Ã¶lÃ§mÉ™diyini iddia edir. DigÉ™r tÉ™nqidÃ§ilÉ™r isÉ™ testin subyektiv olduÄŸunu vÉ™ hakimin qÉ™rÉ™zli ola bilÉ™cÉ™yini iddia edir. Turinq testinin tÉ™siri Turinq testi, sÃ¼ni zÉ™ka tÉ™dqiqatlarÄ± sahÉ™sindÉ™ bÃ¶yÃ¼k tÉ™sirÉ™ malik olmuÅŸdur.',\n",
       "   'Bunlardan bir neÃ§É™si aÅŸaÄŸÄ±da qeyd olunmuÅŸdur: T\\x03rinqtesti vÉ™int\\x03itivyanaÅŸma Modern kompÃ¼terlÉ™rin atasÄ± hesab edilÉ™n Alan Turinq, 1950-ci ildÉ™ \"Mind\" jurnalÄ±nda nÉ™ÅŸr olunan \"Hesablama MaÅŸÄ±nlarÄ± vÉ™ ZÉ™ka\" adlÄ± mÉ™qalÉ™sindÉ™ sÃ¼ni zÉ™ka ilÉ™ baÄŸlÄ± ilk dÃ¼ÅŸÃ¼ncÉ™lÉ™rini ortaya qoymuÅŸdur. Bu mÉ™qalÉ™dÉ™ o, insan vÉ™ kompÃ¼ter arasÄ±nda aparÄ±lan bir testdÉ™n bÉ™hs edir. Bu testdÉ™ bir insan hakim rolunu oynayÄ±r vÉ™ bir insanla bir kompÃ¼terlÉ™ sual-cavab aparÄ±r. Hakim, nÉ™ insanÄ±, nÉ™ dÉ™ kompÃ¼teri gÃ¶rÉ™ bilir.',\n",
       "   'KompÃ¼ter \"ÅŸÃ¼urluluÄŸunun\" da hÉ™r hansÄ± bir hÉ™ddi yoxdur, lakin sÃ¼ni zÉ™kanÄ±n yaradÄ±lmasÄ±na bir sÄ±ra hipofizlÉ™r verilib, bunlara da Turinq testini vÉ™ Nyuel-Saymon hipofizini misal gÃ¶stÉ™rmÉ™k olar. Ona gÃ¶rÉ™ dÉ™ sÃ¼ni zÉ™ka probleminin qoyulmasÄ±na iki cÃ¼r yaxÄ±nlaÅŸmaq olar: YuxarÄ±dan aÅŸaÄŸÄ±ya, semiotik â€“ ekspert sistemlÉ™r, biliklÉ™r vÉ™ mÉ™ntiqi qÉ™rarlar bazasÄ±, yÃ¼ksÉ™ksÉ™viyyÉ™li psixi proseslÉ™ri tÉ™qlid edÉ™n sistemlÉ™rin yaradÄ±lmasÄ±, mÉ™sÉ™lÉ™n, dÃ¼ÅŸÃ¼ncÉ™, danÄ±ÅŸÄ±q, incÉ™sÉ™nÉ™t vÉ™ s.',\n",
       "   'Baxmayaraq ki, Nyuell-Saymon fÉ™rziyyÉ™si, yaxud Turinq testi kimi, bir sÄ±ra fÉ™rziyyÉ™lÉ™r irÉ™li sÃ¼rÃ¼lmÃ¼ÅŸdÃ¼, kompÃ¼terin \"aÄŸlabatan\" sÉ™vviyyÉ™sinÉ™ qÉ™dÉ™r inkiÅŸaf etmÉ™sinin dÉ™qiq sÃ¼butu yoxdur. Robotexnika Robotexnika sahÉ™si vÉ™ sÃ¼ni zÉ™ka elmi bir-biri ilÉ™ sÄ±x É™laqÉ™dardÄ±r. Robotexnikada sÃ¼ni zÉ™kadan istifadÉ™ edib intellektual robotlarÄ±n yaradÄ±lmasÄ± sÃ¼ni zÉ™ka elminin yeni bir sahÉ™sini yaradÄ±r. Ä°ntellektuallÄ±q Ã¼zÉ™rindÉ™ cisimlÉ™ri manipulyasÄ±ya etmÉ™k, olduÄŸu yeri tapmaq, É™traf alÉ™mi tÉ™hlil etmÉ™k vÉ™ hÉ™rÉ™kÉ™ti planlaÅŸdÄ±rmaq Ã¼Ã§Ã¼n iÅŸlÉ™mÉ™k lazÄ±mdÄ±r.',\n",
       "   'Â· Ä°llik RoboCup turnirindÉ™, robotlarÄ±n, futbolun sadÉ™lÉ™ÅŸdirilmiÅŸ formasÄ±nda rÉ™qabÉ™tin aparÄ±lmasÄ±. Â· Virtual sÃ¶hbÉ™t proqramlarÄ±. ChatMaster- Virtual sÃ¶hbÉ™t proqramlarÄ± insan danÄ±ÅŸÄ±ÄŸÄ±nÄ± tÉ™qlid edÉ™n, bir vÉ™ daha Ã§ox insanla sÃ¶hbÉ™t edÉ™n kompÃ¼ter proqramÄ±dÄ±r. Ä°lk belÉ™ proqramlardan biri 1969-cu ildÉ™ Cozef Beyzenbaum tÉ™rÉ™findÉ™n yaradÄ±lan Eliza proqramÄ± olmuÅŸdur. BelÉ™ dÃ¼ÅŸÃ¼nÃ¼lÃ¼r ki, ideal virtual sÃ¶hbÉ™t proqramÄ± Turinq testini keÃ§É™ bilÉ™cÉ™k. Chatmaster Ã¶zÃ¼-Ã¶zÃ¼nÃ¼ Ã¶yrÉ™dÉ™ bilÉ™n, insanla dialoq apara bilÉ™n proqramdÄ±r. Dialoq klaviatura ilÉ™ aparÄ±lÄ±r vÉ™ ekranda É™ks etdirilir.',\n",
       "   'Test, bu sahÉ™dÉ™ bir Ã§ox tÉ™dqiqat vÉ™ inkiÅŸafa ilham vermiÅŸ vÉ™ sÃ¼ni zÉ™kanÄ±n nÉ™ olduÄŸu vÉ™ necÉ™ Ã¶lÃ§Ã¼lmÉ™si lazÄ±m olduÄŸu barÉ™dÉ™ mÃ¼zakirÉ™lÉ™rÉ™ sÉ™bÉ™b olmuÅŸdur. SimvolikyanaÅŸma SÃ¼ni zÉ™ka probleminÉ™ simvolik yanaÅŸma ilk rÉ™qÉ™msal maÅŸÄ±nlarÄ±n yaranmasÄ± dÃ¶vrÃ¼ndÉ™ yaranÄ±b. Bu yanaÅŸmada problemlÉ™rin hÉ™llinin tapÄ±lmasÄ±nÄ±n uÄŸur vÉ™ sÉ™mÉ™rÉ™liyi Ã¼Ã§Ã¼n informasiyanÄ±n abstraktlÄ±ÄŸÄ± vacibdir. MÉ™ntiqiyanaÅŸma SÃ¼ni zÉ™ka sisteminin yaradÄ±lmasÄ±na mÉ™ntiqi yanaÅŸma modellÉ™ÅŸdirilmiÅŸ mÃ¼hakimÉ™yÉ™ É™saslanÄ±r. Teorik É™sas kimi mÉ™ntiqÉ™ É™saslanÄ±r.',\n",
       "   'MÃ¼É™llimsiz Ã¶yrÉ™nmÉ™ cisimlÉ™ri olduÄŸu kimi qavramaÄŸa icazÉ™ verir. MÃ¼É™llim ilÉ™ Ã¶yrÉ™nmÉ™ isÉ™ siniflÉ™ndirmÉ™yÉ™ vÉ™ reqressiv analizÉ™ kÃ¶mÉ™k edir. SiniflÉ™ndirmÉ™ onun Ã¼Ã§Ã¼n istifadÉ™ olunur ki, cismin hansÄ± kateqoriyaya aid olduÄŸu aydÄ±nlaÅŸdÄ±rÄ±lsÄ±n. Reqressiv analiz isÉ™ É™dÉ™di sÄ±ralarda giriÅŸ É™dÉ™dlÉ™rindÉ™n istifadÉ™ olunan funksiya tapÄ±lmasÄ± ilÉ™ Ã§Ä±xÄ±ÅŸ É™dÉ™dinin proqnozlaÅŸmasÄ± Ã¼Ã§Ã¼n istifadÉ™ olunur. TÉ™lim prosesi zamanÄ± maÅŸÄ±nÄ±n dÃ¼zgÃ¼n cavablarÄ± mÃ¼kafatlandÄ±rÄ±lÄ±r, yanlÄ±ÅŸ cavablarÄ± isÉ™ cÉ™zalandÄ±rÄ±lÄ±r. MÃ¼asir dÃ¶vrdÉ™ki sÃ¼ni zÉ™kanÄ±n É™sasÄ± 1956-cÄ± ildÉ™Dartmut KollecindÉ™ keÃ§irilÉ™n konferensiyada qoyulmuÅŸdur.',\n",
       "   'Ars Maqnaya istifadÉ™Ã§i xristianlÄ±q haqqÄ±nda Ã¶z sualÄ±nÄ± daxil edir vÉ™ aparat insanÄ±n kÃ¶mÉ™yi olmadan dÉ™rhal avtomatik olaraq bu suala cavab tapÄ±rdÄ±. Bu aparat mÃ¼sÉ™lmanlarÄ± xristianlaÅŸdÄ±rmaq mÉ™qsÉ™dilÉ™ yaradÄ±lmÄ±ÅŸdÄ±r. MaÅŸÄ±nlarÄ±n tÉ™limi sÉ™rbÉ™st ÅŸÉ™kildÉ™ sÃ¼ni zÉ™ka biliklÉ™rinin vÉ™ onun iÅŸlÉ™mÉ™ proseslÉ™rinin alÄ±nmasÄ±na aiddir. Bu sÃ¼ni zÉ™ka elminin ilk dÃ¶vrlÉ™rindÉ™n mÉ™rkÉ™zi olmuÅŸdur. 1956-cÄ± ildÉ™ Darmund yaz konferansÄ±nda Rey Solomonoff Ã¶z-Ã¶zÃ¼nÃ¼ Ã¶yrÉ™dÉ™ bilÉ™n maÅŸÄ±nÄ±n ola bilÉ™cÉ™yi haqqÄ±nda hesabat yazdÄ± vÉ™ onu \"Ä°ntuitiv nÉ™ticÉ™ maÅŸÄ±nÄ±\" adlandÄ±rdÄ±. MaÅŸÄ±n tÉ™liminin iki nÃ¶vÃ¼ vardÄ±r; mÃ¼É™llimsiz vÉ™ mÃ¼É™llim ilÉ™.',\n",
       "   'AI) â€” insanlarÄ±n vÉ™ ya heyvanlarÄ±n intellektindÉ™n fÉ™rqli olaraq maÅŸÄ±n vÉ™ ya proqram tÉ™minatÄ±nÄ±n intellekti.[1] SÃ¼ni intellekt tÉ™tbiqlÉ™rinÉ™ qabaqcÄ±lveb axtarÄ±ÅŸ mÃ¼hÉ™rriklÉ™ri (Quql axtarÄ±ÅŸ sistemi), tÃ¶vsiyÉ™ sistemlÉ™ri (\"YouTube\", \"Amazon\" vÉ™ \"Netflix\" tÉ™rÉ™findÉ™n istifadÉ™ olunur),virtual assistentlÉ™r (\"Siri\" vÉ™ \"Alexa\" kimi), Ã¶zÃ¼gedÉ™n avtomobillÉ™r (\"Waymo\"), generativ vÉ™ ya yaradÄ±cÄ± alÉ™tlÉ™r (\"ChatGPT\" vÉ™ sÃ¼ni intellekt incÉ™sÉ™nÉ™ti), strateji oyunlarda (ÅŸahmat vÉ™ Qo kimi) É™n yÃ¼ksÉ™k sÉ™viyyÉ™dÉ™ yarÄ±ÅŸmaq daxildir.[2] SÃ¼ni intellekt 1956-cÄ± ildÉ™ akademik fÉ™nn kimi tÉ™sis edilmiÅŸdir.] Bu sahÉ™ optimizm dÃ¶vrlÉ™rindÉ™n keÃ§miÅŸ, sonradan mÉ™yusluq vÉ™ maliyyÉ™ itkisi yaÅŸamÄ±ÅŸ, lakin 2012-ci ildÉ™n sonra dÉ™rin Ã¶yrÉ™nmÉ™ bÃ¼tÃ¼n É™vvÉ™lki sÃ¼ni intellekt Ã¼sullarÄ±nÄ± Ã¼stÉ™lÉ™miÅŸ,[8] belÉ™liklÉ™ bu sahÉ™dÉ™ maliyyÉ™lÉ™ÅŸmÉ™ vÉ™ maraqda bÃ¶yÃ¼k artÄ±m mÃ¼ÅŸahidÉ™ edilmiÅŸdir.']],\n",
       " 'uris': None,\n",
       " 'data': None}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection.query(query_embeddings=query_embedding,\n",
    "                 n_results=10,\n",
    "                 include=[\"documents\", \"distances\", \"metadatas\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
