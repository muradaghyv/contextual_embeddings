{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XLMRobertaModel, XLMRobertaTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa302228a9084385b0edc591badc8333",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  98%|#########7| 1.09G/1.12G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "model = XLMRobertaModel.from_pretrained(\"xlm-roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: I am a Machine Learning Engineer.\n",
      "Sentence embedding: [ 0.07968017  0.11534335  0.08195748 -0.02582322  0.08123931 -0.04061352\n",
      "  0.02149741 -0.03822808  0.10016774 -0.20736234  0.00568451  0.19195543\n",
      " -0.05058713  0.03820678  0.03627932]\n",
      "\n",
      "Sentence: I have studied at Baku Higher Oil School.\n",
      "Sentence embedding: [ 0.10620356  0.09569398  0.0708335   0.00350476  0.0746572  -0.0956919\n",
      "  0.01693573 -0.0437999   0.13914847 -0.16263598 -0.00573601  0.18464628\n",
      " -0.06672061  0.02892233  0.05468758]\n",
      "\n",
      "\n",
      "Similarity between sentences is: 0.9982\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "embeddings = []\n",
    "\n",
    "sentences = [\"I am a Machine Learning Engineer.\", \n",
    "            \"I have studied at Baku Higher Oil School.\"]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for sent in sentences:\n",
    "        inputs = tokenizer(sent, return_tensors=\"pt\", padding=True)\n",
    "        outputs = model(**inputs)\n",
    "        sent_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "        embeddings.append(sent_embedding)\n",
    "        print(f\"Sentence: {sent}\")\n",
    "        print(f\"Sentence embedding: {sent_embedding[0, :15].numpy()}\\n\")\n",
    "\n",
    "\n",
    "similarity = torch.nn.functional.cosine_similarity(embeddings[0], embeddings[1])\n",
    "print(f\"\\nSimilarity between sentences is: {similarity.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\n",
    "    \"HuggingFace is a company based in Paris and New York\", add_special_tokens=False, return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "predicted_token_class_ids = logits.argmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Azerbaijani Sentence Pairs:\n",
      "Sentence: Mən maşın öyrənmə mühəndisiyəm.\n",
      "Embedding (first 5 values): [ 0.03006841  0.08905941  0.02749911 -0.05828931  0.13824546]\n",
      "Sentence: Mənim işimin adı maşın öyrənmə mühəndisliyidir.\n",
      "Embedding (first 5 values): [ 0.05652007  0.00162089  0.03152019 -0.08628289  0.15883191]\n",
      "Similarity between similar sentences: 0.9963\n",
      "\n",
      "Sentence: Mən maşın öyrənmə mühəndisiyəm.\n",
      "Embedding (first 5 values): [ 0.03006841  0.08905941  0.02749911 -0.05828931  0.13824546]\n",
      "Sentence: Mən Bakı Ali Neft Məktəbində oxumuşam.\n",
      "Embedding (first 5 values): [ 0.10853648  0.05083771  0.02417384 -0.06507861  0.1261565 ]\n",
      "Similarity between different sentences: 0.9960\n",
      "\n",
      "\n",
      "English Sentence Pairs:\n",
      "Sentence: I am a Machine Learning Engineer.\n",
      "Embedding (first 5 values): [-0.02279879  0.0478951   0.04590836 -0.00515924  0.16044599]\n",
      "Sentence: I have studied at Baku Higher Oil School.\n",
      "Embedding (first 5 values): [0.00707844 0.03284997 0.00859101 0.01044608 0.1108994 ]\n",
      "Similarity between similar sentences: 0.9973\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Mean pooling function\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    # First element of model_output contains all token embeddings\n",
    "    token_embeddings = model_output.last_hidden_state\n",
    "    \n",
    "    # Create attention mask in the same shape as token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    \n",
    "    # Sum token embeddings and divide by the total token count\n",
    "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    \n",
    "    # Return mean-pooled embedding\n",
    "    return sum_embeddings / sum_mask\n",
    "\n",
    "# Test sentences\n",
    "azerbaijani_pairs = [\n",
    "    # Similar meaning\n",
    "    [\"Mən maşın öyrənmə mühəndisiyəm.\", \"Mənim işimin adı maşın öyrənmə mühəndisliyidir.\"],\n",
    "    \n",
    "    # Different meaning\n",
    "    [\"Mən maşın öyrənmə mühəndisiyəm.\", \"Mən Bakı Ali Neft Məktəbində oxumuşam.\"]\n",
    "]\n",
    "\n",
    "english_pairs = [\n",
    "    # Different meaning\n",
    "    [\"I am a Machine Learning Engineer.\", \"I have studied at Baku Higher Oil School.\"]\n",
    "]\n",
    "\n",
    "# Process all sentence pairs\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, pairs in enumerate([azerbaijani_pairs, english_pairs]):\n",
    "        language = \"Azerbaijani\" if i == 0 else \"English\"\n",
    "        print(f\"\\n{language} Sentence Pairs:\")\n",
    "        \n",
    "        for j, pair in enumerate(pairs):\n",
    "            # Process both sentences in the pair\n",
    "            embeddings = []\n",
    "            for sent in pair:\n",
    "                # Tokenize sentences\n",
    "                encoded_input = tokenizer(sent, padding=True, truncation=True, return_tensors='pt')\n",
    "                \n",
    "                # Compute token embeddings\n",
    "                outputs = model(**encoded_input)\n",
    "                \n",
    "                # Apply mean pooling\n",
    "                embedding = mean_pooling(outputs, encoded_input['attention_mask'])\n",
    "                embeddings.append(embedding)\n",
    "                \n",
    "                # Print first 5 values for inspection\n",
    "                print(f\"Sentence: {sent}\")\n",
    "                print(f\"Embedding (first 5 values): {embedding[0, :5].numpy()}\")\n",
    "            \n",
    "            # Compute similarity\n",
    "            similarity = torch.nn.functional.cosine_similarity(embeddings[0], embeddings[1])\n",
    "            similarity_type = \"similar\" if j == 0 else \"different\"\n",
    "            print(f\"Similarity between {similarity_type} sentences: {similarity.item():.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load a multilingual sentence transformer\n",
    "model = SentenceTransformer('distiluse-base-multilingual-cased-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity: 0.3408\n"
     ]
    }
   ],
   "source": [
    "# Get embeddings\n",
    "embedding1 = model.encode(\"Çox acam və yemək yeməliyəm.\")\n",
    "embedding2 = model.encode(\"İnsan çox acdıqda yemək yeməlidir.\")\n",
    "\n",
    "# Calculate similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "similarity = cosine_similarity([embedding1], [embedding2])[0][0]\n",
    "print(f\"Similarity: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
