{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cab3124b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f080558f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/home/murad/Documents/self-study/contextual_embeddings/data/openai_results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e8a19a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"/home/murad/Documents/self-study/contextual_embeddings/data/openai_results_mined_HN\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b603d370",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_DIR = \"/home/murad/Documents/self-study/contextual_embeddings/data/openai_results_mined_HN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8cf475f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for doc_file in os.listdir(data_path):\n",
    "#     input_file = os.path.join(data_path, doc_file)\n",
    "#     input_file = os.path.abspath(input_file)\n",
    "\n",
    "#     output_file = f\"{output_DIR}/mined_HN_{doc_file}\"\n",
    "#     # output_file = os.path.abspath(output_file)\n",
    "\n",
    "#     print(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a8600a6",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 22716.94it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 23172.95it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 19953.87it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 25125.62it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 13839.54it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 29599.89it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 18233.46it/s]\n",
      "inferencing embedding for corpus (number=0)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 166, in find_knn_neg\n",
      "    p_vecs = model.encode(corpus)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 295, in encode\n",
      "    return super().encode(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py\", line 266, in encode\n",
      "    return self.encode_single_device(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 394, in encode_single_device\n",
      "    inputs_batch = self.tokenizer.pad(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3453, in pad\n",
      "    if isinstance(encoded_inputs, (list, tuple)) and isinstance(encoded_inputs[0], Mapping):\n",
      "IndexError: list index out of range\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 13995.01it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 17043.09it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 24750.02it/s]\n",
      "inferencing embedding for corpus (number=3)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 11312.52it/s]\n",
      "inferencing embedding for corpus (number=3)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 20570.40it/s]\n",
      "inferencing embedding for corpus (number=3)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 18507.00it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 26800.66it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 18616.53it/s]\n",
      "inferencing embedding for corpus (number=0)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 166, in find_knn_neg\n",
      "    p_vecs = model.encode(corpus)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 295, in encode\n",
      "    return super().encode(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py\", line 266, in encode\n",
      "    return self.encode_single_device(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 394, in encode_single_device\n",
      "    inputs_batch = self.tokenizer.pad(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3453, in pad\n",
      "    if isinstance(encoded_inputs, (list, tuple)) and isinstance(encoded_inputs[0], Mapping):\n",
      "IndexError: list index out of range\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 17068.52it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 22067.54it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 29803.20it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 17967.89it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 24376.04it/s]\n",
      "inferencing embedding for corpus (number=22)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=5)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|██████████████████████| 30/30 [00:00<00:00, 9754.20it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 35001.15it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 36792.14it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 21557.16it/s]\n",
      "inferencing embedding for corpus (number=7)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=2)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 20627.72it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 30578.16it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 25896.09it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 30563.30it/s]\n",
      "inferencing embedding for corpus (number=20)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=5)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 18578.05it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 12406.74it/s]\n",
      "inferencing embedding for corpus (number=7)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=2)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 25632.33it/s]\n",
      "inferencing embedding for corpus (number=21)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=5)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 21826.39it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 19694.65it/s]\n",
      "inferencing embedding for corpus (number=7)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=2)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|██████████████████████| 30/30 [00:00<00:00, 8691.66it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 16690.43it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 30563.30it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 16901.16it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 25890.77it/s]\n",
      "inferencing embedding for corpus (number=19)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=5)--------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 168, in find_knn_neg\n",
      "    q_vecs = model.encode_queries(queries)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 211, in encode_queries\n",
      "    return super().encode_queries(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py\", line 183, in encode_queries\n",
      "    return self.encode(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 295, in encode\n",
      "    return super().encode(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py\", line 266, in encode\n",
      "    return self.encode_single_device(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 375, in encode_single_device\n",
      "    inputs_batch = self.tokenizer(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3021, in __call__\n",
      "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3109, in _call_one\n",
      "    return self.batch_encode_plus(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3311, in batch_encode_plus\n",
      "    return self._batch_encode_plus(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py\", line 529, in _batch_encode_plus\n",
      "    encodings = self._tokenizer.encode_batch(\n",
      "TypeError: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 18175.52it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 19287.11it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 28174.90it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 14219.59it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 18758.07it/s]\n",
      "inferencing embedding for corpus (number=7)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=2)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 14461.46it/s]\n",
      "inferencing embedding for corpus (number=5)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 39383.14it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 19716.25it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 18385.32it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 19781.34it/s]\n",
      "inferencing embedding for corpus (number=0)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 166, in find_knn_neg\n",
      "    p_vecs = model.encode(corpus)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 295, in encode\n",
      "    return super().encode(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py\", line 266, in encode\n",
      "    return self.encode_single_device(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 394, in encode_single_device\n",
      "    inputs_batch = self.tokenizer.pad(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3453, in pad\n",
      "    if isinstance(encoded_inputs, (list, tuple)) and isinstance(encoded_inputs[0], Mapping):\n",
      "IndexError: list index out of range\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 23020.33it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 17690.02it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 22533.87it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 19415.08it/s]\n",
      "inferencing embedding for corpus (number=14)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=4)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 15748.33it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 23638.76it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 16169.25it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 18859.28it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 21262.10it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 16190.06it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 20936.63it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 19436.07it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 14295.51it/s]\n",
      "inferencing embedding for corpus (number=3)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 20357.40it/s]\n",
      "inferencing embedding for corpus (number=3)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|████████████████████| 30/30 [00:00<00:00, 109607.25it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 17858.23it/s]\n",
      "inferencing embedding for corpus (number=3)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 13332.18it/s]\n",
      "inferencing embedding for corpus (number=3)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|████████████████████| 30/30 [00:00<00:00, 104596.11it/s]\n",
      "inferencing embedding for corpus (number=3)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 25512.80it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 18201.81it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 15667.93it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|████████████████████| 30/30 [00:00<00:00, 141381.03it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 23625.44it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 29475.08it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 27176.92it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 16513.01it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 32388.45it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 15903.58it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 32388.45it/s]\n",
      "inferencing embedding for corpus (number=0)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 166, in find_knn_neg\n",
      "    p_vecs = model.encode(corpus)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 295, in encode\n",
      "    return super().encode(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py\", line 266, in encode\n",
      "    return self.encode_single_device(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 394, in encode_single_device\n",
      "    inputs_batch = self.tokenizer.pad(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3453, in pad\n",
      "    if isinstance(encoded_inputs, (list, tuple)) and isinstance(encoded_inputs[0], Mapping):\n",
      "IndexError: list index out of range\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 16092.74it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|██████████████████████| 30/30 [00:00<00:00, 8719.97it/s]\n",
      "inferencing embedding for corpus (number=28)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=7)--------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 168, in find_knn_neg\n",
      "    q_vecs = model.encode_queries(queries)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 211, in encode_queries\n",
      "    return super().encode_queries(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py\", line 183, in encode_queries\n",
      "    return self.encode(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 295, in encode\n",
      "    return super().encode(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py\", line 266, in encode\n",
      "    return self.encode_single_device(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 375, in encode_single_device\n",
      "    inputs_batch = self.tokenizer(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3021, in __call__\n",
      "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3109, in _call_one\n",
      "    return self.batch_encode_plus(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3311, in batch_encode_plus\n",
      "    return self._batch_encode_plus(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py\", line 529, in _batch_encode_plus\n",
      "    encodings = self._tokenizer.encode_batch(\n",
      "TypeError: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 25543.87it/s]\n",
      "inferencing embedding for corpus (number=5)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 21871.91it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 19143.33it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 19484.22it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 19281.20it/s]\n",
      "inferencing embedding for corpus (number=3)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 25742.45it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 13653.33it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 15988.45it/s]\n",
      "inferencing embedding for corpus (number=18)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=5)--------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 168, in find_knn_neg\n",
      "    q_vecs = model.encode_queries(queries)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 211, in encode_queries\n",
      "    return super().encode_queries(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py\", line 183, in encode_queries\n",
      "    return self.encode(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 295, in encode\n",
      "    return super().encode(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py\", line 266, in encode\n",
      "    return self.encode_single_device(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 375, in encode_single_device\n",
      "    inputs_batch = self.tokenizer(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3021, in __call__\n",
      "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3109, in _call_one\n",
      "    return self.batch_encode_plus(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3311, in batch_encode_plus\n",
      "    return self._batch_encode_plus(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py\", line 529, in _batch_encode_plus\n",
      "    encodings = self._tokenizer.encode_batch(\n",
      "TypeError: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 26738.02it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 23258.62it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 26468.05it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 66859.26it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 18975.89it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 20593.96it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 16976.41it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 20842.99it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 23669.89it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 21352.30it/s]\n",
      "inferencing embedding for corpus (number=3)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 24451.83it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 16082.45it/s]\n",
      "inferencing embedding for corpus (number=3)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 17837.98it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 19236.99it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 17303.23it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 24624.09it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 20148.78it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 26473.62it/s]\n",
      "inferencing embedding for corpus (number=19)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=5)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 14279.29it/s]\n",
      "inferencing embedding for corpus (number=5)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 13294.15it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 25933.45it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 21549.77it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|██████████████████████| 30/30 [00:00<00:00, 8630.85it/s]\n",
      "inferencing embedding for corpus (number=7)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=2)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 18800.11it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 13589.93it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 14819.12it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 15207.77it/s]\n",
      "inferencing embedding for corpus (number=20)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=5)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 19076.58it/s]\n",
      "inferencing embedding for corpus (number=5)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 16855.88it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 26789.25it/s]\n",
      "inferencing embedding for corpus (number=3)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 18439.20it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 42625.04it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 18938.76it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 20281.93it/s]\n",
      "inferencing embedding for corpus (number=3)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 26176.23it/s]\n",
      "inferencing embedding for corpus (number=3)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 20798.20it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 15246.47it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 37560.93it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 44620.26it/s]\n",
      "inferencing embedding for corpus (number=3)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 16683.79it/s]\n",
      "inferencing embedding for corpus (number=2)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 16240.21it/s]\n",
      "inferencing embedding for corpus (number=3)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 30415.55it/s]\n",
      "inferencing embedding for corpus (number=7)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=2)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 17008.53it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 16439.66it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 18178.15it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 17418.21it/s]\n",
      "inferencing embedding for corpus (number=3)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 34257.86it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 18460.85it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|████████████████████| 30/30 [00:00<00:00, 247208.49it/s]\n",
      "inferencing embedding for corpus (number=5)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 22828.21it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 17650.32it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 25901.42it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 21535.02it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 19397.12it/s]\n",
      "inferencing embedding for corpus (number=3)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 24995.85it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 18244.04it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|██████████████████████| 30/30 [00:00<00:00, 5843.55it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 21664.79it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 15787.84it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 40960.00it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 26507.08it/s]\n",
      "inferencing embedding for corpus (number=3)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 15742.41it/s]\n",
      "inferencing embedding for corpus (number=3)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 20295.02it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 19192.97it/s]\n",
      "inferencing embedding for corpus (number=20)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=5)--------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 168, in find_knn_neg\n",
      "    q_vecs = model.encode_queries(queries)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 211, in encode_queries\n",
      "    return super().encode_queries(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py\", line 183, in encode_queries\n",
      "    return self.encode(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 295, in encode\n",
      "    return super().encode(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py\", line 266, in encode\n",
      "    return self.encode_single_device(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 375, in encode_single_device\n",
      "    inputs_batch = self.tokenizer(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3021, in __call__\n",
      "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3109, in _call_one\n",
      "    return self.batch_encode_plus(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3311, in batch_encode_plus\n",
      "    return self._batch_encode_plus(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py\", line 529, in _batch_encode_plus\n",
      "    encodings = self._tokenizer.encode_batch(\n",
      "TypeError: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 20207.02it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 16413.92it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 24461.34it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 18047.78it/s]\n",
      "inferencing embedding for corpus (number=3)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 26909.56it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 39482.00it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 30563.30it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 15532.54it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 21807.47it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 17225.07it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 28174.90it/s]\n",
      "inferencing embedding for corpus (number=20)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=5)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 22986.69it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 40629.36it/s]\n",
      "inferencing embedding for corpus (number=12)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=3)--------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 168, in find_knn_neg\n",
      "    q_vecs = model.encode_queries(queries)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 211, in encode_queries\n",
      "    return super().encode_queries(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py\", line 183, in encode_queries\n",
      "    return self.encode(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 295, in encode\n",
      "    return super().encode(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py\", line 266, in encode\n",
      "    return self.encode_single_device(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 375, in encode_single_device\n",
      "    inputs_batch = self.tokenizer(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3021, in __call__\n",
      "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3109, in _call_one\n",
      "    return self.batch_encode_plus(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3311, in batch_encode_plus\n",
      "    return self._batch_encode_plus(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py\", line 529, in _batch_encode_plus\n",
      "    encodings = self._tokenizer.encode_batch(\n",
      "TypeError: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 21151.31it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 38362.54it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 16951.25it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 19137.51it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 32530.80it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 14179.53it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 18170.27it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 168, in find_knn_neg\n",
      "    q_vecs = model.encode_queries(queries)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 211, in encode_queries\n",
      "    return super().encode_queries(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py\", line 183, in encode_queries\n",
      "    return self.encode(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 295, in encode\n",
      "    return super().encode(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py\", line 266, in encode\n",
      "    return self.encode_single_device(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 375, in encode_single_device\n",
      "    inputs_batch = self.tokenizer(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3021, in __call__\n",
      "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3109, in _call_one\n",
      "    return self.batch_encode_plus(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3311, in batch_encode_plus\n",
      "    return self._batch_encode_plus(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py\", line 529, in _batch_encode_plus\n",
      "    encodings = self._tokenizer.encode_batch(\n",
      "TypeError: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 17947.39it/s]\n",
      "inferencing embedding for corpus (number=20)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=5)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 27407.78it/s]\n",
      "inferencing embedding for corpus (number=18)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=5)--------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 168, in find_knn_neg\n",
      "    q_vecs = model.encode_queries(queries)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 211, in encode_queries\n",
      "    return super().encode_queries(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py\", line 183, in encode_queries\n",
      "    return self.encode(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 295, in encode\n",
      "    return super().encode(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py\", line 266, in encode\n",
      "    return self.encode_single_device(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 375, in encode_single_device\n",
      "    inputs_batch = self.tokenizer(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3021, in __call__\n",
      "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3109, in _call_one\n",
      "    return self.batch_encode_plus(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3311, in batch_encode_plus\n",
      "    return self._batch_encode_plus(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py\", line 529, in _batch_encode_plus\n",
      "    encodings = self._tokenizer.encode_batch(\n",
      "TypeError: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 22840.65it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 31799.12it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 15613.49it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 38550.59it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 18222.90it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 41831.49it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 14614.30it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 20463.35it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 28416.69it/s]\n",
      "inferencing embedding for corpus (number=6)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=2)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 17675.11it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 19016.04it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 24451.83it/s]\n",
      "inferencing embedding for corpus (number=0)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 166, in find_knn_neg\n",
      "    p_vecs = model.encode(corpus)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 295, in encode\n",
      "    return super().encode(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py\", line 266, in encode\n",
      "    return self.encode_single_device(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 394, in encode_single_device\n",
      "    inputs_batch = self.tokenizer.pad(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3453, in pad\n",
      "    if isinstance(encoded_inputs, (list, tuple)) and isinstance(encoded_inputs[0], Mapping):\n",
      "IndexError: list index out of range\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 11937.11it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 13140.05it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 16171.33it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 13940.74it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 17787.55it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 64594.00it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 18068.51it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 168, in find_knn_neg\n",
      "    q_vecs = model.encode_queries(queries)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 211, in encode_queries\n",
      "    return super().encode_queries(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py\", line 183, in encode_queries\n",
      "    return self.encode(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 295, in encode\n",
      "    return super().encode(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py\", line 266, in encode\n",
      "    return self.encode_single_device(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 375, in encode_single_device\n",
      "    inputs_batch = self.tokenizer(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3021, in __call__\n",
      "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3109, in _call_one\n",
      "    return self.batch_encode_plus(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3311, in batch_encode_plus\n",
      "    return self._batch_encode_plus(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py\", line 529, in _batch_encode_plus\n",
      "    encodings = self._tokenizer.encode_batch(\n",
      "TypeError: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 15935.81it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 23151.63it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 23853.86it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 168, in find_knn_neg\n",
      "    q_vecs = model.encode_queries(queries)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 211, in encode_queries\n",
      "    return super().encode_queries(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py\", line 183, in encode_queries\n",
      "    return self.encode(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 295, in encode\n",
      "    return super().encode(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py\", line 266, in encode\n",
      "    return self.encode_single_device(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 375, in encode_single_device\n",
      "    inputs_batch = self.tokenizer(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3021, in __call__\n",
      "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3109, in _call_one\n",
      "    return self.batch_encode_plus(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3311, in batch_encode_plus\n",
      "    return self._batch_encode_plus(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py\", line 529, in _batch_encode_plus\n",
      "    encodings = self._tokenizer.encode_batch(\n",
      "TypeError: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 10413.73it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 14528.24it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 24091.35it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 22505.66it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 19853.13it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 17727.40it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 31433.70it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 19793.79it/s]\n",
      "inferencing embedding for corpus (number=3)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 13859.36it/s]\n",
      "inferencing embedding for corpus (number=16)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=4)--------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 168, in find_knn_neg\n",
      "    q_vecs = model.encode_queries(queries)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 211, in encode_queries\n",
      "    return super().encode_queries(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py\", line 183, in encode_queries\n",
      "    return self.encode(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 295, in encode\n",
      "    return super().encode(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py\", line 266, in encode\n",
      "    return self.encode_single_device(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 375, in encode_single_device\n",
      "    inputs_batch = self.tokenizer(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3021, in __call__\n",
      "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3109, in _call_one\n",
      "    return self.batch_encode_plus(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3311, in batch_encode_plus\n",
      "    return self._batch_encode_plus(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py\", line 529, in _batch_encode_plus\n",
      "    encodings = self._tokenizer.encode_batch(\n",
      "TypeError: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 16305.45it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|██████████████████████| 30/30 [00:00<00:00, 7813.53it/s]\n",
      "inferencing embedding for corpus (number=5)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 13614.92it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 19969.71it/s]\n",
      "inferencing embedding for corpus (number=16)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=4)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 14081.15it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 32589.77it/s]\n",
      "inferencing embedding for corpus (number=12)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=3)--------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 168, in find_knn_neg\n",
      "    q_vecs = model.encode_queries(queries)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 211, in encode_queries\n",
      "    return super().encode_queries(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py\", line 183, in encode_queries\n",
      "    return self.encode(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 295, in encode\n",
      "    return super().encode(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py\", line 266, in encode\n",
      "    return self.encode_single_device(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 375, in encode_single_device\n",
      "    inputs_batch = self.tokenizer(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3021, in __call__\n",
      "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3109, in _call_one\n",
      "    return self.batch_encode_plus(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3311, in batch_encode_plus\n",
      "    return self._batch_encode_plus(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py\", line 529, in _batch_encode_plus\n",
      "    encodings = self._tokenizer.encode_batch(\n",
      "TypeError: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 31823.25it/s]\n",
      "inferencing embedding for corpus (number=3)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|██████████████████████| 30/30 [00:00<00:00, 6708.74it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 21013.55it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 26630.50it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 15479.04it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 21932.91it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 25784.66it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 14906.90it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 23899.17it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 16650.67it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 29235.39it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 14479.76it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 28952.86it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 20010.99it/s]\n",
      "inferencing embedding for corpus (number=0)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 166, in find_knn_neg\n",
      "    p_vecs = model.encode(corpus)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 295, in encode\n",
      "    return super().encode(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py\", line 266, in encode\n",
      "    return self.encode_single_device(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 394, in encode_single_device\n",
      "    inputs_batch = self.tokenizer.pad(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3453, in pad\n",
      "    if isinstance(encoded_inputs, (list, tuple)) and isinstance(encoded_inputs[0], Mapping):\n",
      "IndexError: list index out of range\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 17532.27it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 22377.58it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 14005.91it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 25795.23it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 19307.83it/s]\n",
      "inferencing embedding for corpus (number=19)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=6)--------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 168, in find_knn_neg\n",
      "    q_vecs = model.encode_queries(queries)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 211, in encode_queries\n",
      "    return super().encode_queries(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py\", line 183, in encode_queries\n",
      "    return self.encode(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 295, in encode\n",
      "    return super().encode(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py\", line 266, in encode\n",
      "    return self.encode_single_device(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 375, in encode_single_device\n",
      "    inputs_batch = self.tokenizer(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3021, in __call__\n",
      "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3109, in _call_one\n",
      "    return self.batch_encode_plus(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3311, in batch_encode_plus\n",
      "    return self._batch_encode_plus(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py\", line 529, in _batch_encode_plus\n",
      "    encodings = self._tokenizer.encode_batch(\n",
      "TypeError: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 22207.75it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 28623.55it/s]\n",
      "inferencing embedding for corpus (number=0)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 166, in find_knn_neg\n",
      "    p_vecs = model.encode(corpus)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 295, in encode\n",
      "    return super().encode(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py\", line 266, in encode\n",
      "    return self.encode_single_device(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 394, in encode_single_device\n",
      "    inputs_batch = self.tokenizer.pad(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3453, in pad\n",
      "    if isinstance(encoded_inputs, (list, tuple)) and isinstance(encoded_inputs[0], Mapping):\n",
      "IndexError: list index out of range\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 43195.72it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 21583.04it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 16523.85it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 19737.90it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 19679.25it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 17916.72it/s]\n",
      "inferencing embedding for corpus (number=0)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 166, in find_knn_neg\n",
      "    p_vecs = model.encode(corpus)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 295, in encode\n",
      "    return super().encode(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py\", line 266, in encode\n",
      "    return self.encode_single_device(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 394, in encode_single_device\n",
      "    inputs_batch = self.tokenizer.pad(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3453, in pad\n",
      "    if isinstance(encoded_inputs, (list, tuple)) and isinstance(encoded_inputs[0], Mapping):\n",
      "IndexError: list index out of range\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 17610.79it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 20161.69it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 24399.67it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 18487.97it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 26335.10it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 20654.81it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 19950.71it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 17225.07it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 18186.03it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 27906.21it/s]\n",
      "inferencing embedding for corpus (number=21)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=5)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 13753.32it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 18619.28it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 30283.78it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 33235.37it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 16589.21it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 25564.63it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 19750.29it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 16835.58it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 32239.08it/s]\n",
      "inferencing embedding for corpus (number=3)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 32665.92it/s]\n",
      "inferencing embedding for corpus (number=19)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=5)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 18133.61it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 17251.04it/s]\n",
      "inferencing embedding for corpus (number=0)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 166, in find_knn_neg\n",
      "    p_vecs = model.encode(corpus)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 295, in encode\n",
      "    return super().encode(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py\", line 266, in encode\n",
      "    return self.encode_single_device(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 394, in encode_single_device\n",
      "    inputs_batch = self.tokenizer.pad(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3453, in pad\n",
      "    if isinstance(encoded_inputs, (list, tuple)) and isinstance(encoded_inputs[0], Mapping):\n",
      "IndexError: list index out of range\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 18550.66it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 18337.09it/s]\n",
      "inferencing embedding for corpus (number=0)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 166, in find_knn_neg\n",
      "    p_vecs = model.encode(corpus)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 295, in encode\n",
      "    return super().encode(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py\", line 266, in encode\n",
      "    return self.encode_single_device(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 394, in encode_single_device\n",
      "    inputs_batch = self.tokenizer.pad(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3453, in pad\n",
      "    if isinstance(encoded_inputs, (list, tuple)) and isinstance(encoded_inputs[0], Mapping):\n",
      "IndexError: list index out of range\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 27776.85it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 15627.06it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 15773.99it/s]\n",
      "inferencing embedding for corpus (number=8)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=2)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 21244.15it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 22461.46it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 17647.84it/s]\n",
      "inferencing embedding for corpus (number=20)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=5)--------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 168, in find_knn_neg\n",
      "    q_vecs = model.encode_queries(queries)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 211, in encode_queries\n",
      "    return super().encode_queries(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py\", line 183, in encode_queries\n",
      "    return self.encode(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 295, in encode\n",
      "    return super().encode(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py\", line 266, in encode\n",
      "    return self.encode_single_device(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 375, in encode_single_device\n",
      "    inputs_batch = self.tokenizer(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3021, in __call__\n",
      "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3109, in _call_one\n",
      "    return self.batch_encode_plus(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3311, in batch_encode_plus\n",
      "    return self._batch_encode_plus(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py\", line 529, in _batch_encode_plus\n",
      "    encodings = self._tokenizer.encode_batch(\n",
      "TypeError: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 19648.52it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 19290.07it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|████████████████████| 30/30 [00:00<00:00, 151967.54it/s]\n",
      "inferencing embedding for corpus (number=3)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 30189.33it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 34370.15it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 12394.52it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 57508.74it/s]\n",
      "inferencing embedding for corpus (number=31)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=9)--------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 168, in find_knn_neg\n",
      "    q_vecs = model.encode_queries(queries)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 211, in encode_queries\n",
      "    return super().encode_queries(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py\", line 183, in encode_queries\n",
      "    return self.encode(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 295, in encode\n",
      "    return super().encode(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py\", line 266, in encode\n",
      "    return self.encode_single_device(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 375, in encode_single_device\n",
      "    inputs_batch = self.tokenizer(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3021, in __call__\n",
      "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3109, in _call_one\n",
      "    return self.batch_encode_plus(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3311, in batch_encode_plus\n",
      "    return self._batch_encode_plus(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py\", line 529, in _batch_encode_plus\n",
      "    encodings = self._tokenizer.encode_batch(\n",
      "TypeError: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 18744.10it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 22028.91it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 13844.11it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 19421.07it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 18539.73it/s]\n",
      "inferencing embedding for corpus (number=5)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 13601.68it/s]\n",
      "inferencing embedding for corpus (number=22)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=5)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 16892.08it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 20805.08it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|████████████████████| 30/30 [00:00<00:00, 234318.66it/s]\n",
      "inferencing embedding for corpus (number=3)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 24045.31it/s]\n",
      "inferencing embedding for corpus (number=3)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 21959.71it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 17131.26it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 39606.27it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 23660.99it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 27083.32it/s]\n",
      "inferencing embedding for corpus (number=3)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 19021.79it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 26434.69it/s]\n",
      "inferencing embedding for corpus (number=3)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 28578.04it/s]\n",
      "inferencing embedding for corpus (number=3)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 28130.81it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 18199.18it/s]\n",
      "inferencing embedding for corpus (number=20)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=5)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 27449.63it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 31138.11it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 50860.60it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 17965.32it/s]\n",
      "inferencing embedding for corpus (number=5)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=2)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 14195.52it/s]\n",
      "inferencing embedding for corpus (number=3)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 30885.89it/s]\n",
      "inferencing embedding for corpus (number=11)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=3)--------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 168, in find_knn_neg\n",
      "    q_vecs = model.encode_queries(queries)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 211, in encode_queries\n",
      "    return super().encode_queries(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py\", line 183, in encode_queries\n",
      "    return self.encode(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 295, in encode\n",
      "    return super().encode(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py\", line 266, in encode\n",
      "    return self.encode_single_device(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 375, in encode_single_device\n",
      "    inputs_batch = self.tokenizer(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3021, in __call__\n",
      "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3109, in _call_one\n",
      "    return self.batch_encode_plus(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3311, in batch_encode_plus\n",
      "    return self._batch_encode_plus(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py\", line 529, in _batch_encode_plus\n",
      "    encodings = self._tokenizer.encode_batch(\n",
      "TypeError: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 27147.60it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 25848.22it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 32330.20it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 16928.44it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 30225.59it/s]\n",
      "inferencing embedding for corpus (number=3)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 58716.34it/s]\n",
      "inferencing embedding for corpus (number=20)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=5)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 25970.92it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 28455.25it/s]\n",
      "inferencing embedding for corpus (number=3)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 18089.29it/s]\n",
      "inferencing embedding for corpus (number=5)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=2)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 32042.05it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 21395.87it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 15121.87it/s]\n",
      "inferencing embedding for corpus (number=0)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 166, in find_knn_neg\n",
      "    p_vecs = model.encode(corpus)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 295, in encode\n",
      "    return super().encode(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py\", line 266, in encode\n",
      "    return self.encode_single_device(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 394, in encode_single_device\n",
      "    inputs_batch = self.tokenizer.pad(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3453, in pad\n",
      "    if isinstance(encoded_inputs, (list, tuple)) and isinstance(encoded_inputs[0], Mapping):\n",
      "IndexError: list index out of range\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 22692.36it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 30261.93it/s]\n",
      "inferencing embedding for corpus (number=0)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 166, in find_knn_neg\n",
      "    p_vecs = model.encode(corpus)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 295, in encode\n",
      "    return super().encode(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py\", line 266, in encode\n",
      "    return self.encode_single_device(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 394, in encode_single_device\n",
      "    inputs_batch = self.tokenizer.pad(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3453, in pad\n",
      "    if isinstance(encoded_inputs, (list, tuple)) and isinstance(encoded_inputs[0], Mapping):\n",
      "IndexError: list index out of range\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 21612.70it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 47880.18it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 13700.91it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 24552.02it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 61022.85it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 29006.25it/s]\n",
      "inferencing embedding for corpus (number=8)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=2)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 16766.04it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 23207.14it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 15857.48it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 20410.24it/s]\n",
      "inferencing embedding for corpus (number=12)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=3)--------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 168, in find_knn_neg\n",
      "    q_vecs = model.encode_queries(queries)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 211, in encode_queries\n",
      "    return super().encode_queries(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py\", line 183, in encode_queries\n",
      "    return self.encode(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 295, in encode\n",
      "    return super().encode(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py\", line 266, in encode\n",
      "    return self.encode_single_device(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 375, in encode_single_device\n",
      "    inputs_batch = self.tokenizer(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3021, in __call__\n",
      "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3109, in _call_one\n",
      "    return self.batch_encode_plus(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3311, in batch_encode_plus\n",
      "    return self._batch_encode_plus(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py\", line 529, in _batch_encode_plus\n",
      "    encodings = self._tokenizer.encode_batch(\n",
      "TypeError: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 14869.90it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 34904.06it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 16652.87it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 23271.52it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 20317.96it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 19569.07it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 21830.17it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 20560.31it/s]\n",
      "inferencing embedding for corpus (number=3)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 30459.72it/s]\n",
      "inferencing embedding for corpus (number=18)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=5)--------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 168, in find_knn_neg\n",
      "    q_vecs = model.encode_queries(queries)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 211, in encode_queries\n",
      "    return super().encode_queries(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py\", line 183, in encode_queries\n",
      "    return self.encode(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 295, in encode\n",
      "    return super().encode(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py\", line 266, in encode\n",
      "    return self.encode_single_device(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 375, in encode_single_device\n",
      "    inputs_batch = self.tokenizer(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3021, in __call__\n",
      "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3109, in _call_one\n",
      "    return self.batch_encode_plus(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3311, in batch_encode_plus\n",
      "    return self._batch_encode_plus(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py\", line 529, in _batch_encode_plus\n",
      "    encodings = self._tokenizer.encode_batch(\n",
      "TypeError: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 26252.69it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 29845.62it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 51931.13it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 18186.03it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 15823.58it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 19846.86it/s]\n",
      "inferencing embedding for corpus (number=3)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 27171.05it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 96199.63it/s]\n",
      "inferencing embedding for corpus (number=3)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 13880.76it/s]\n",
      "inferencing embedding for corpus (number=7)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=2)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 23228.56it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 50231.19it/s]\n",
      "inferencing embedding for corpus (number=18)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=6)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 12377.45it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 26709.64it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 24774.39it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 42224.54it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 18374.58it/s]\n",
      "inferencing embedding for corpus (number=19)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=5)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 25055.58it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 168, in find_knn_neg\n",
      "    q_vecs = model.encode_queries(queries)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 211, in encode_queries\n",
      "    return super().encode_queries(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py\", line 183, in encode_queries\n",
      "    return self.encode(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 295, in encode\n",
      "    return super().encode(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py\", line 266, in encode\n",
      "    return self.encode_single_device(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 375, in encode_single_device\n",
      "    inputs_batch = self.tokenizer(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3021, in __call__\n",
      "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3109, in _call_one\n",
      "    return self.batch_encode_plus(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3311, in batch_encode_plus\n",
      "    return self._batch_encode_plus(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py\", line 529, in _batch_encode_plus\n",
      "    encodings = self._tokenizer.encode_batch(\n",
      "TypeError: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 18032.26it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 15281.65it/s]\n",
      "inferencing embedding for corpus (number=20)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=5)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 27147.60it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 25949.50it/s]\n",
      "inferencing embedding for corpus (number=3)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 14563.56it/s]\n",
      "inferencing embedding for corpus (number=20)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=5)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 19663.87it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 25965.56it/s]\n",
      "inferencing embedding for corpus (number=12)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=3)--------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 168, in find_knn_neg\n",
      "    q_vecs = model.encode_queries(queries)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 211, in encode_queries\n",
      "    return super().encode_queries(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py\", line 183, in encode_queries\n",
      "    return self.encode(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 295, in encode\n",
      "    return super().encode(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py\", line 266, in encode\n",
      "    return self.encode_single_device(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 375, in encode_single_device\n",
      "    inputs_batch = self.tokenizer(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3021, in __call__\n",
      "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3109, in _call_one\n",
      "    return self.batch_encode_plus(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3311, in batch_encode_plus\n",
      "    return self._batch_encode_plus(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py\", line 529, in _batch_encode_plus\n",
      "    encodings = self._tokenizer.encode_batch(\n",
      "TypeError: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 87563.76it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 16813.08it/s]\n",
      "inferencing embedding for corpus (number=18)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=5)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 78154.73it/s]\n",
      "inferencing embedding for corpus (number=3)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 18170.27it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 26214.40it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 22770.38it/s]\n",
      "inferencing embedding for corpus (number=18)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=5)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 19912.82it/s]\n",
      "inferencing embedding for corpus (number=3)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 168, in find_knn_neg\n",
      "    q_vecs = model.encode_queries(queries)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 211, in encode_queries\n",
      "    return super().encode_queries(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py\", line 183, in encode_queries\n",
      "    return self.encode(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 295, in encode\n",
      "    return super().encode(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py\", line 266, in encode\n",
      "    return self.encode_single_device(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 375, in encode_single_device\n",
      "    inputs_batch = self.tokenizer(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3021, in __call__\n",
      "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3109, in _call_one\n",
      "    return self.batch_encode_plus(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3311, in batch_encode_plus\n",
      "    return self._batch_encode_plus(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py\", line 529, in _batch_encode_plus\n",
      "    encodings = self._tokenizer.encode_batch(\n",
      "TypeError: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 18517.90it/s]\n",
      "inferencing embedding for corpus (number=8)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=2)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 24896.94it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 14193.92it/s]\n",
      "inferencing embedding for corpus (number=20)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=5)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 24614.46it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 15693.33it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 17682.56it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 14146.05it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 27588.06it/s]\n",
      "inferencing embedding for corpus (number=7)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=2)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 25135.66it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 26636.14it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 15136.43it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 21226.23it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 23768.25it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 20553.60it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 12842.33it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 20891.44it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 27600.16it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 31223.11it/s]\n",
      "inferencing embedding for corpus (number=3)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 16914.79it/s]\n",
      "inferencing embedding for corpus (number=3)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|██████████████████████| 30/30 [00:00<00:00, 6503.13it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 43554.56it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 14213.16it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 18655.17it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 17084.74it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 39309.32it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 21601.57it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 22199.92it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 17495.71it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 25668.94it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 18027.09it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 80197.02it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 28333.51it/s]\n",
      "inferencing embedding for corpus (number=3)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 17246.32it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 20049.25it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 23016.12it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 16418.20it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 25000.82it/s]\n",
      "inferencing embedding for corpus (number=17)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=5)--------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 168, in find_knn_neg\n",
      "    q_vecs = model.encode_queries(queries)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 211, in encode_queries\n",
      "    return super().encode_queries(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py\", line 183, in encode_queries\n",
      "    return self.encode(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 295, in encode\n",
      "    return super().encode(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py\", line 266, in encode\n",
      "    return self.encode_single_device(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 375, in encode_single_device\n",
      "    inputs_batch = self.tokenizer(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3021, in __call__\n",
      "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3109, in _call_one\n",
      "    return self.batch_encode_plus(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3311, in batch_encode_plus\n",
      "    return self._batch_encode_plus(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py\", line 529, in _batch_encode_plus\n",
      "    encodings = self._tokenizer.encode_batch(\n",
      "TypeError: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 16630.86it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 25768.81it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 46311.79it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|██████████████████████| 30/30 [00:00<00:00, 6858.67it/s]\n",
      "inferencing embedding for corpus (number=17)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=5)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 21119.36it/s]\n",
      "inferencing embedding for corpus (number=7)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=2)--------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 168, in find_knn_neg\n",
      "    q_vecs = model.encode_queries(queries)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 211, in encode_queries\n",
      "    return super().encode_queries(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py\", line 183, in encode_queries\n",
      "    return self.encode(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 295, in encode\n",
      "    return super().encode(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py\", line 266, in encode\n",
      "    return self.encode_single_device(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 375, in encode_single_device\n",
      "    inputs_batch = self.tokenizer(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3021, in __call__\n",
      "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3109, in _call_one\n",
      "    return self.batch_encode_plus(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3311, in batch_encode_plus\n",
      "    return self._batch_encode_plus(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py\", line 529, in _batch_encode_plus\n",
      "    encodings = self._tokenizer.encode_batch(\n",
      "TypeError: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 20420.18it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 24916.66it/s]\n",
      "inferencing embedding for corpus (number=0)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 166, in find_knn_neg\n",
      "    p_vecs = model.encode(corpus)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 295, in encode\n",
      "    return super().encode(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py\", line 266, in encode\n",
      "    return self.encode_single_device(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 394, in encode_single_device\n",
      "    inputs_batch = self.tokenizer.pad(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3453, in pad\n",
      "    if isinstance(encoded_inputs, (list, tuple)) and isinstance(encoded_inputs[0], Mapping):\n",
      "IndexError: list index out of range\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 19108.45it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 24532.88it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 32346.82it/s]\n",
      "inferencing embedding for corpus (number=5)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 18953.02it/s]\n",
      "inferencing embedding for corpus (number=0)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 166, in find_knn_neg\n",
      "    p_vecs = model.encode(corpus)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 295, in encode\n",
      "    return super().encode(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py\", line 266, in encode\n",
      "    return self.encode_single_device(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 394, in encode_single_device\n",
      "    inputs_batch = self.tokenizer.pad(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3453, in pad\n",
      "    if isinstance(encoded_inputs, (list, tuple)) and isinstance(encoded_inputs[0], Mapping):\n",
      "IndexError: list index out of range\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 16615.49it/s]\n",
      "inferencing embedding for corpus (number=3)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 16418.20it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 17401.34it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 13333.59it/s]\n",
      "inferencing embedding for corpus (number=0)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 166, in find_knn_neg\n",
      "    p_vecs = model.encode(corpus)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 295, in encode\n",
      "    return super().encode(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py\", line 266, in encode\n",
      "    return self.encode_single_device(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 394, in encode_single_device\n",
      "    inputs_batch = self.tokenizer.pad(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3453, in pad\n",
      "    if isinstance(encoded_inputs, (list, tuple)) and isinstance(encoded_inputs[0], Mapping):\n",
      "IndexError: list index out of range\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 68947.46it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 13860.89it/s]\n",
      "inferencing embedding for corpus (number=3)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 11760.83it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 14905.13it/s]\n",
      "inferencing embedding for corpus (number=5)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 16910.24it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 32206.07it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 17810.21it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n"
     ]
    }
   ],
   "source": [
    "for doc_file in os.listdir(data_path):\n",
    "    input_file_raw = os.path.join(data_path, doc_file)\n",
    "    input_file = os.path.abspath(input_file_raw)\n",
    "\n",
    "    output_file = f\"{output_DIR}/mined_HN_{doc_file}\"\n",
    "    if not os.path.exists(output_file):\n",
    "    \n",
    "        ! python ../../FlagEmbedding/scripts/hn_mine.py \\\n",
    "            --input_file {input_file} \\\n",
    "            --output_file {output_file} \\\n",
    "            --range_for_sampling 5-50 \\\n",
    "            --negative_number 10 \\\n",
    "            --embedder_name_or_path BAAI/bge-m3 \\\n",
    "            --use_gpu_for_searching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93679716",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8a8c0843",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
