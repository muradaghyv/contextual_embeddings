{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cab3124b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f080558f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/home/murad/Documents/self-study/contextual_embeddings/data/openai_results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e8a19a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"/home/murad/Documents/self-study/contextual_embeddings/data/openai_results_mined_HN\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b603d370",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_DIR = \"/home/murad/Documents/self-study/contextual_embeddings/data/openai_results_mined_HN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8cf475f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for doc_file in os.listdir(data_path):\n",
    "#     input_file = os.path.join(data_path, doc_file)\n",
    "#     input_file = os.path.abspath(input_file)\n",
    "\n",
    "#     output_file = f\"{output_DIR}/mined_HN_{doc_file}\"\n",
    "#     # output_file = os.path.abspath(output_file)\n",
    "\n",
    "#     print(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8600a6",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 16390.40it/s]\n",
      "inferencing embedding for corpus (number=8)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=2)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 20894.91it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 57535.03it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 25627.11it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 22235.22it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 17934.60it/s]\n",
      "inferencing embedding for corpus (number=8)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=2)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 17835.45it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 31496.65it/s]\n",
      "inferencing embedding for corpus (number=18)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=5)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 14383.76it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 25858.84it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 20801.64it/s]\n",
      "inferencing embedding for corpus (number=20)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=5)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 23280.13it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 14820.86it/s]\n",
      "inferencing embedding for corpus (number=3)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 17365.32it/s]\n",
      "inferencing embedding for corpus (number=3)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 19400.11it/s]\n",
      "inferencing embedding for corpus (number=8)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=2)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 24961.14it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 16288.56it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 34146.30it/s]\n",
      "inferencing embedding for corpus (number=16)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=4)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 22160.82it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 28435.96it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|██████████████████████| 30/30 [00:00<00:00, 8196.80it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|██████████████████████| 30/30 [00:00<00:00, 6794.23it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 17568.99it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 24528.09it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 17432.68it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 22235.22it/s]\n",
      "inferencing embedding for corpus (number=7)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=2)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 13498.08it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|██████████████████████| 30/30 [00:00<00:00, 9706.79it/s]\n",
      "inferencing embedding for corpus (number=19)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=5)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 14368.98it/s]\n",
      "inferencing embedding for corpus (number=20)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=5)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 17087.06it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 35982.02it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 42871.93it/s]\n",
      "inferencing embedding for corpus (number=15)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=4)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 28005.59it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 16280.13it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 18816.98it/s]\n",
      "inferencing embedding for corpus (number=12)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=3)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 27980.68it/s]\n",
      "inferencing embedding for corpus (number=14)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=5)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 18201.81it/s]\n",
      "inferencing embedding for corpus (number=7)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=2)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 27900.03it/s]\n",
      "inferencing embedding for corpus (number=20)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=5)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 16539.05it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 12395.74it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|████████████████████| 30/30 [00:00<00:00, 266023.51it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 17348.56it/s]\n",
      "inferencing embedding for corpus (number=19)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=5)--------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 168, in find_knn_neg\n",
      "    q_vecs = model.encode_queries(queries)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 211, in encode_queries\n",
      "    return super().encode_queries(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py\", line 183, in encode_queries\n",
      "    return self.encode(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 295, in encode\n",
      "    return super().encode(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py\", line 266, in encode\n",
      "    return self.encode_single_device(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 375, in encode_single_device\n",
      "    inputs_batch = self.tokenizer(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3021, in __call__\n",
      "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3109, in _call_one\n",
      "    return self.batch_encode_plus(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3311, in batch_encode_plus\n",
      "    return self._batch_encode_plus(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py\", line 529, in _batch_encode_plus\n",
      "    encodings = self._tokenizer.encode_batch(\n",
      "TypeError: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 26863.60it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 16204.65it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 19517.47it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 22803.39it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 13945.38it/s]\n",
      "inferencing embedding for corpus (number=7)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=2)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 22079.16it/s]\n",
      "inferencing embedding for corpus (number=5)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 25538.69it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 16337.20it/s]\n",
      "inferencing embedding for corpus (number=8)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=2)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 18561.60it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 26949.91it/s]\n",
      "inferencing embedding for corpus (number=8)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=2)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 20695.58it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 25658.47it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 15187.58it/s]\n",
      "inferencing embedding for corpus (number=21)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=5)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 17248.68it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|████████████████████| 30/30 [00:00<00:00, 162360.15it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 20912.27it/s]\n",
      "inferencing embedding for corpus (number=9)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=2)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 19688.49it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 40047.46it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|██████████████████████| 30/30 [00:00<00:00, 5536.55it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 16356.31it/s]\n",
      "inferencing embedding for corpus (number=19)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=5)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 17610.79it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 15707.04it/s]\n",
      "inferencing embedding for corpus (number=3)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 19225.23it/s]\n",
      "inferencing embedding for corpus (number=19)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=5)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 17080.10it/s]\n",
      "inferencing embedding for corpus (number=19)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=5)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 19152.07it/s]\n",
      "inferencing embedding for corpus (number=3)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 21055.74it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 47215.43it/s]\n",
      "inferencing embedding for corpus (number=3)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 14463.12it/s]\n",
      "inferencing embedding for corpus (number=3)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 18318.40it/s]\n",
      "inferencing embedding for corpus (number=9)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=2)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 16467.62it/s]\n",
      "inferencing embedding for corpus (number=3)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 20563.67it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 32396.79it/s]\n",
      "inferencing embedding for corpus (number=21)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=5)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 17633.00it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 35059.66it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 13960.85it/s]\n",
      "inferencing embedding for corpus (number=19)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=5)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 17052.33it/s]\n",
      "inferencing embedding for corpus (number=8)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=2)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 22333.89it/s]\n",
      "inferencing embedding for corpus (number=24)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=6)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 31599.48it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 66823.75it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 18921.67it/s]\n",
      "inferencing embedding for corpus (number=20)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=5)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 16980.99it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 20781.03it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|████████████████████| 30/30 [00:00<00:00, 193285.90it/s]\n",
      "inferencing embedding for corpus (number=9)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=3)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|██████████████████████| 30/30 [00:00<00:00, 5358.76it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 43404.32it/s]\n",
      "inferencing embedding for corpus (number=17)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=5)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 59890.11it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 19523.53it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 20252.55it/s]\n",
      "inferencing embedding for corpus (number=28)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=7)--------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 168, in find_knn_neg\n",
      "    q_vecs = model.encode_queries(queries)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 211, in encode_queries\n",
      "    return super().encode_queries(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py\", line 183, in encode_queries\n",
      "    return self.encode(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 295, in encode\n",
      "    return super().encode(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py\", line 266, in encode\n",
      "    return self.encode_single_device(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 375, in encode_single_device\n",
      "    inputs_batch = self.tokenizer(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3021, in __call__\n",
      "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3109, in _call_one\n",
      "    return self.batch_encode_plus(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3311, in batch_encode_plus\n",
      "    return self._batch_encode_plus(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py\", line 529, in _batch_encode_plus\n",
      "    encodings = self._tokenizer.encode_batch(\n",
      "TypeError: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 21055.74it/s]\n",
      "inferencing embedding for corpus (number=5)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 13886.89it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 21229.82it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 52891.60it/s]\n",
      "inferencing embedding for corpus (number=20)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=5)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 18893.26it/s]\n",
      "inferencing embedding for corpus (number=3)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 20377.19it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 32581.34it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 22704.64it/s]\n",
      "inferencing embedding for corpus (number=18)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=5)--------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 168, in find_knn_neg\n",
      "    q_vecs = model.encode_queries(queries)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 211, in encode_queries\n",
      "    return super().encode_queries(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py\", line 183, in encode_queries\n",
      "    return self.encode(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 295, in encode\n",
      "    return super().encode(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py\", line 266, in encode\n",
      "    return self.encode_single_device(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 375, in encode_single_device\n",
      "    inputs_batch = self.tokenizer(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3021, in __call__\n",
      "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3109, in _call_one\n",
      "    return self.batch_encode_plus(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3311, in batch_encode_plus\n",
      "    return self._batch_encode_plus(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py\", line 529, in _batch_encode_plus\n",
      "    encodings = self._tokenizer.encode_batch(\n",
      "TypeError: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 15369.38it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 16650.67it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 11372.84it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|██████████████████████| 30/30 [00:00<00:00, 3748.26it/s]\n",
      "inferencing embedding for corpus (number=21)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=5)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 27697.36it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 20563.67it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 14040.29it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 32598.22it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 17822.82it/s]\n",
      "inferencing embedding for corpus (number=8)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=2)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 19941.22it/s]\n",
      "inferencing embedding for corpus (number=16)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=4)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 37249.59it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 21129.99it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 87078.98it/s]\n",
      "inferencing embedding for corpus (number=21)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=5)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 58146.54it/s]\n",
      "inferencing embedding for corpus (number=3)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 26024.64it/s]\n",
      "inferencing embedding for corpus (number=3)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 15461.92it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 22857.24it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 18220.26it/s]\n",
      "inferencing embedding for corpus (number=19)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=5)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 26692.64it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 46586.12it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 24291.34it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 18597.27it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|██████████████████████| 30/30 [00:00<00:00, 8219.83it/s]\n",
      "inferencing embedding for corpus (number=7)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=2)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|██████████████████████| 30/30 [00:00<00:00, 7994.23it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 16768.27it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 13182.73it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 21194.06it/s]\n",
      "inferencing embedding for corpus (number=8)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=2)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 13282.92it/s]\n",
      "inferencing embedding for corpus (number=8)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=2)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 23362.26it/s]\n",
      "inferencing embedding for corpus (number=5)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 19146.24it/s]\n",
      "inferencing embedding for corpus (number=20)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=5)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 31300.78it/s]\n",
      "inferencing embedding for corpus (number=3)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 14883.97it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 16554.28it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 20832.64it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 14685.94it/s]\n",
      "inferencing embedding for corpus (number=3)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|██████████████████████| 30/30 [00:00<00:00, 8088.78it/s]\n",
      "inferencing embedding for corpus (number=3)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 13601.68it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 22009.64it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 18283.80it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 56450.93it/s]\n",
      "inferencing embedding for corpus (number=3)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 12280.80it/s]\n",
      "inferencing embedding for corpus (number=2)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 19281.20it/s]\n",
      "inferencing embedding for corpus (number=7)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=2)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 23431.87it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 20288.47it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 28308.01it/s]\n",
      "inferencing embedding for corpus (number=3)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 18913.14it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 17500.57it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 34100.03it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 19284.16it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 28442.39it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 31952.54it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 24063.71it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 17797.61it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 48063.07it/s]\n",
      "inferencing embedding for corpus (number=12)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=3)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 18131.00it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 18741.30it/s]\n",
      "inferencing embedding for corpus (number=12)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=3)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 30305.66it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 18331.75it/s]\n",
      "inferencing embedding for corpus (number=3)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 22824.07it/s]\n",
      "inferencing embedding for corpus (number=3)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 17241.59it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 30269.21it/s]\n",
      "inferencing embedding for corpus (number=20)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=5)--------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 168, in find_knn_neg\n",
      "    q_vecs = model.encode_queries(queries)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 211, in encode_queries\n",
      "    return super().encode_queries(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py\", line 183, in encode_queries\n",
      "    return self.encode(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 295, in encode\n",
      "    return super().encode(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py\", line 266, in encode\n",
      "    return self.encode_single_device(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 375, in encode_single_device\n",
      "    inputs_batch = self.tokenizer(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3021, in __call__\n",
      "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3109, in _call_one\n",
      "    return self.batch_encode_plus(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3311, in batch_encode_plus\n",
      "    return self._batch_encode_plus(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py\", line 529, in _batch_encode_plus\n",
      "    encodings = self._tokenizer.encode_batch(\n",
      "TypeError: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 21454.24it/s]\n",
      "inferencing embedding for corpus (number=11)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=3)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 18680.10it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 14157.19it/s]\n",
      "inferencing embedding for corpus (number=15)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=4)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 76959.71it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 17695.00it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 26062.37it/s]\n",
      "inferencing embedding for corpus (number=3)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 18326.41it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 20074.84it/s]\n",
      "inferencing embedding for corpus (number=22)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=5)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 14391.98it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 36610.16it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 16476.25it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 12929.42it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 27782.98it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 22401.48it/s]\n",
      "inferencing embedding for corpus (number=20)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=5)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 52891.60it/s]\n",
      "inferencing embedding for corpus (number=12)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=3)--------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 168, in find_knn_neg\n",
      "    q_vecs = model.encode_queries(queries)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 211, in encode_queries\n",
      "    return super().encode_queries(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py\", line 183, in encode_queries\n",
      "    return self.encode(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 295, in encode\n",
      "    return super().encode(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py\", line 266, in encode\n",
      "    return self.encode_single_device(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 375, in encode_single_device\n",
      "    inputs_batch = self.tokenizer(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3021, in __call__\n",
      "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3109, in _call_one\n",
      "    return self.batch_encode_plus(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3311, in batch_encode_plus\n",
      "    return self._batch_encode_plus(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py\", line 529, in _batch_encode_plus\n",
      "    encodings = self._tokenizer.encode_batch(\n",
      "TypeError: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 15156.48it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 17822.82it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 20354.11it/s]\n",
      "inferencing embedding for corpus (number=18)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=5)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 22610.80it/s]\n",
      "inferencing embedding for corpus (number=12)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=3)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 26777.85it/s]\n",
      "inferencing embedding for corpus (number=8)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=2)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 20919.22it/s]\n",
      "inferencing embedding for corpus (number=112)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=30)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 37639.58it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 35049.89it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 17947.39it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 16992.45it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 168, in find_knn_neg\n",
      "    q_vecs = model.encode_queries(queries)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 211, in encode_queries\n",
      "    return super().encode_queries(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py\", line 183, in encode_queries\n",
      "    return self.encode(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 295, in encode\n",
      "    return super().encode(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py\", line 266, in encode\n",
      "    return self.encode_single_device(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 375, in encode_single_device\n",
      "    inputs_batch = self.tokenizer(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3021, in __call__\n",
      "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3109, in _call_one\n",
      "    return self.batch_encode_plus(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3311, in batch_encode_plus\n",
      "    return self._batch_encode_plus(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py\", line 529, in _batch_encode_plus\n",
      "    encodings = self._tokenizer.encode_batch(\n",
      "TypeError: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 21137.09it/s]\n",
      "inferencing embedding for corpus (number=8)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=2)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 20275.40it/s]\n",
      "inferencing embedding for corpus (number=11)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=3)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 17077.79it/s]\n",
      "inferencing embedding for corpus (number=73)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=17)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 28519.75it/s]\n",
      "inferencing embedding for corpus (number=8)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=2)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 17827.87it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 14915.73it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|██████████████████████| 30/30 [00:00<00:00, 7098.16it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 22848.94it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 23858.38it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 16810.84it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 19457.11it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 16907.97it/s]\n",
      "inferencing embedding for corpus (number=6)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=2)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 18058.14it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 12184.48it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 18040.02it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 31238.61it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 31544.03it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 14248.57it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 17825.35it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 168, in find_knn_neg\n",
      "    q_vecs = model.encode_queries(queries)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 211, in encode_queries\n",
      "    return super().encode_queries(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py\", line 183, in encode_queries\n",
      "    return self.encode(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 295, in encode\n",
      "    return super().encode(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py\", line 266, in encode\n",
      "    return self.encode_single_device(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 375, in encode_single_device\n",
      "    inputs_batch = self.tokenizer(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3021, in __call__\n",
      "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3109, in _call_one\n",
      "    return self.batch_encode_plus(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3311, in batch_encode_plus\n",
      "    return self._batch_encode_plus(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py\", line 529, in _batch_encode_plus\n",
      "    encodings = self._tokenizer.encode_batch(\n",
      "TypeError: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 18724.57it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 21414.08it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 17863.30it/s]\n",
      "inferencing embedding for corpus (number=19)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=5)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 15857.48it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 168, in find_knn_neg\n",
      "    q_vecs = model.encode_queries(queries)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 211, in encode_queries\n",
      "    return super().encode_queries(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py\", line 183, in encode_queries\n",
      "    return self.encode(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 295, in encode\n",
      "    return super().encode(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py\", line 266, in encode\n",
      "    return self.encode_single_device(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 375, in encode_single_device\n",
      "    inputs_batch = self.tokenizer(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3021, in __call__\n",
      "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3109, in _call_one\n",
      "    return self.batch_encode_plus(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3311, in batch_encode_plus\n",
      "    return self._batch_encode_plus(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py\", line 529, in _batch_encode_plus\n",
      "    encodings = self._tokenizer.encode_batch(\n",
      "TypeError: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 30298.37it/s]\n",
      "inferencing embedding for corpus (number=21)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=5)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 25606.25it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 19676.17it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 12812.25it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 33815.94it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 32572.90it/s]\n",
      "inferencing embedding for corpus (number=20)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=5)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 20926.18it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 20049.25it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 19803.14it/s]\n",
      "inferencing embedding for corpus (number=16)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=4)--------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 168, in find_knn_neg\n",
      "    q_vecs = model.encode_queries(queries)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 211, in encode_queries\n",
      "    return super().encode_queries(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py\", line 183, in encode_queries\n",
      "    return self.encode(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 295, in encode\n",
      "    return super().encode(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py\", line 266, in encode\n",
      "    return self.encode_single_device(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 375, in encode_single_device\n",
      "    inputs_batch = self.tokenizer(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3021, in __call__\n",
      "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3109, in _call_one\n",
      "    return self.batch_encode_plus(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3311, in batch_encode_plus\n",
      "    return self._batch_encode_plus(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py\", line 529, in _batch_encode_plus\n",
      "    encodings = self._tokenizer.encode_batch(\n",
      "TypeError: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 15365.63it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 20288.47it/s]\n",
      "inferencing embedding for corpus (number=5)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 24314.81it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 17461.72it/s]\n",
      "inferencing embedding for corpus (number=12)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=3)--------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 168, in find_knn_neg\n",
      "    q_vecs = model.encode_queries(queries)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 211, in encode_queries\n",
      "    return super().encode_queries(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py\", line 183, in encode_queries\n",
      "    return self.encode(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 295, in encode\n",
      "    return super().encode(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py\", line 266, in encode\n",
      "    return self.encode_single_device(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 375, in encode_single_device\n",
      "    inputs_batch = self.tokenizer(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3021, in __call__\n",
      "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3109, in _call_one\n",
      "    return self.batch_encode_plus(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3311, in batch_encode_plus\n",
      "    return self._batch_encode_plus(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py\", line 529, in _batch_encode_plus\n",
      "    encodings = self._tokenizer.encode_batch(\n",
      "TypeError: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 19682.33it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 19602.60it/s]\n",
      "inferencing embedding for corpus (number=19)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=5)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 24828.16it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 18671.78it/s]\n",
      "inferencing embedding for corpus (number=20)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=5)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 20036.48it/s]\n",
      "inferencing embedding for corpus (number=22)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=5)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 70217.14it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|██████████████████████| 30/30 [00:00<00:00, 5581.24it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 24638.56it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 10602.39it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 38740.49it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 16969.54it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 30117.07it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 16626.47it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 28493.91it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 65741.44it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 14342.77it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 20999.52it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 28308.01it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 17103.32it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 18935.91it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 37729.87it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 22457.45it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 15990.48it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 12507.86it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 24990.89it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 18366.53it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 24137.56it/s]\n",
      "inferencing embedding for corpus (number=20)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=5)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 19502.34it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 20363.99it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 58771.19it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|██████████████████████| 30/30 [00:00<00:00, 7163.22it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 23607.71it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 17850.63it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 20753.61it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 16708.16it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 25517.97it/s]\n",
      "inferencing embedding for corpus (number=20)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=5)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 15998.62it/s]\n",
      "inferencing embedding for corpus (number=11)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=3)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 25160.79it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 35434.84it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 20010.99it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 28833.44it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 31599.48it/s]\n",
      "inferencing embedding for corpus (number=20)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=5)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 21137.09it/s]\n",
      "inferencing embedding for corpus (number=3)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 17189.77it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 16602.34it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 18646.88it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 21498.23it/s]\n",
      "inferencing embedding for corpus (number=11)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=3)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 16165.10it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 10997.13it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 29174.38it/s]\n",
      "inferencing embedding for corpus (number=20)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=5)--------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 168, in find_knn_neg\n",
      "    q_vecs = model.encode_queries(queries)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 211, in encode_queries\n",
      "    return super().encode_queries(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py\", line 183, in encode_queries\n",
      "    return self.encode(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 295, in encode\n",
      "    return super().encode(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py\", line 266, in encode\n",
      "    return self.encode_single_device(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 375, in encode_single_device\n",
      "    inputs_batch = self.tokenizer(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3021, in __call__\n",
      "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3109, in _call_one\n",
      "    return self.batch_encode_plus(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3311, in batch_encode_plus\n",
      "    return self._batch_encode_plus(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py\", line 529, in _batch_encode_plus\n",
      "    encodings = self._tokenizer.encode_batch(\n",
      "TypeError: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 20753.61it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 19688.49it/s]\n",
      "inferencing embedding for corpus (number=3)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 18011.61it/s]\n",
      "inferencing embedding for corpus (number=8)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=2)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 15636.77it/s]\n",
      "inferencing embedding for corpus (number=8)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=2)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 17608.33it/s]\n",
      "inferencing embedding for corpus (number=20)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=5)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 26395.87it/s]\n",
      "inferencing embedding for corpus (number=20)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=5)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 33095.51it/s]\n",
      "inferencing embedding for corpus (number=31)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=9)--------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 168, in find_knn_neg\n",
      "    q_vecs = model.encode_queries(queries)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 211, in encode_queries\n",
      "    return super().encode_queries(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py\", line 183, in encode_queries\n",
      "    return self.encode(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 295, in encode\n",
      "    return super().encode(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py\", line 266, in encode\n",
      "    return self.encode_single_device(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 375, in encode_single_device\n",
      "    inputs_batch = self.tokenizer(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3021, in __call__\n",
      "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3109, in _call_one\n",
      "    return self.batch_encode_plus(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3311, in batch_encode_plus\n",
      "    return self._batch_encode_plus(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py\", line 529, in _batch_encode_plus\n",
      "    encodings = self._tokenizer.encode_batch(\n",
      "TypeError: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 21172.66it/s]\n",
      "inferencing embedding for corpus (number=8)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=2)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 47716.77it/s]\n",
      "inferencing embedding for corpus (number=68)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=19)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 22696.45it/s]\n",
      "inferencing embedding for corpus (number=20)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=5)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 24179.31it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 18063.32it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 15471.43it/s]\n",
      "inferencing embedding for corpus (number=19)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=5)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 18244.04it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 20929.66it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 25297.37it/s]\n",
      "inferencing embedding for corpus (number=20)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=5)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 26203.48it/s]\n",
      "inferencing embedding for corpus (number=5)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 21337.82it/s]\n",
      "inferencing embedding for corpus (number=20)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=5)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 26357.17it/s]\n",
      "inferencing embedding for corpus (number=19)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=5)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 27147.60it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 15222.49it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 22409.46it/s]\n",
      "inferencing embedding for corpus (number=3)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 25125.62it/s]\n",
      "inferencing embedding for corpus (number=3)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 21251.33it/s]\n",
      "inferencing embedding for corpus (number=22)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=6)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 27943.40it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 13786.47it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 24296.03it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 69098.91it/s]\n",
      "inferencing embedding for corpus (number=12)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=3)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 18802.92it/s]\n",
      "inferencing embedding for corpus (number=20)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=5)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 14638.10it/s]\n",
      "inferencing embedding for corpus (number=3)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 20964.53it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 21457.90it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|██████████████████████| 30/30 [00:00<00:00, 6845.24it/s]\n",
      "inferencing embedding for corpus (number=31)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=8)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 18321.07it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 18705.09it/s]\n",
      "inferencing embedding for corpus (number=8)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=2)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 19673.10it/s]\n",
      "inferencing embedding for corpus (number=5)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=2)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 27176.92it/s]\n",
      "inferencing embedding for corpus (number=3)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 21890.94it/s]\n",
      "inferencing embedding for corpus (number=18)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=5)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 89942.19it/s]\n",
      "inferencing embedding for corpus (number=11)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=3)--------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 168, in find_knn_neg\n",
      "    q_vecs = model.encode_queries(queries)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 211, in encode_queries\n",
      "    return super().encode_queries(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py\", line 183, in encode_queries\n",
      "    return self.encode(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 295, in encode\n",
      "    return super().encode(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py\", line 266, in encode\n",
      "    return self.encode_single_device(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 375, in encode_single_device\n",
      "    inputs_batch = self.tokenizer(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3021, in __call__\n",
      "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3109, in _call_one\n",
      "    return self.batch_encode_plus(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3311, in batch_encode_plus\n",
      "    return self._batch_encode_plus(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py\", line 529, in _batch_encode_plus\n",
      "    encodings = self._tokenizer.encode_batch(\n",
      "TypeError: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 29167.62it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 22774.50it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 20490.00it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 15163.79it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 22129.64it/s]\n",
      "inferencing embedding for corpus (number=20)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=5)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 21076.90it/s]\n",
      "inferencing embedding for corpus (number=9)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=2)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 16109.22it/s]\n",
      "inferencing embedding for corpus (number=19)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=5)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 21535.02it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 24985.93it/s]\n",
      "inferencing embedding for corpus (number=11)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=3)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 17707.45it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 18984.48it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|██████████████████████| 30/30 [00:00<00:00, 9241.95it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 32025.74it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|████████████████████| 30/30 [00:00<00:00, 143640.55it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 37911.76it/s]\n",
      "inferencing embedding for corpus (number=12)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=3)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 19304.87it/s]\n",
      "inferencing embedding for corpus (number=21)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=5)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 22627.07it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 19840.61it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 19837.48it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 13824.34it/s]\n",
      "inferencing embedding for corpus (number=12)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=3)--------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 168, in find_knn_neg\n",
      "    q_vecs = model.encode_queries(queries)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 211, in encode_queries\n",
      "    return super().encode_queries(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py\", line 183, in encode_queries\n",
      "    return self.encode(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 295, in encode\n",
      "    return super().encode(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py\", line 266, in encode\n",
      "    return self.encode_single_device(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 375, in encode_single_device\n",
      "    inputs_batch = self.tokenizer(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3021, in __call__\n",
      "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3109, in _call_one\n",
      "    return self.batch_encode_plus(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3311, in batch_encode_plus\n",
      "    return self._batch_encode_plus(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py\", line 529, in _batch_encode_plus\n",
      "    encodings = self._tokenizer.encode_batch(\n",
      "TypeError: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 10366.54it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 16584.83it/s]\n",
      "inferencing embedding for corpus (number=12)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=3)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 28807.03it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 48657.82it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 23894.63it/s]\n",
      "inferencing embedding for corpus (number=8)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=2)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 56705.33it/s]\n",
      "inferencing embedding for corpus (number=15)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=4)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 14737.54it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 28979.53it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 14084.30it/s]\n",
      "inferencing embedding for corpus (number=3)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 17772.47it/s]\n",
      "inferencing embedding for corpus (number=18)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=5)--------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 168, in find_knn_neg\n",
      "    q_vecs = model.encode_queries(queries)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 211, in encode_queries\n",
      "    return super().encode_queries(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py\", line 183, in encode_queries\n",
      "    return self.encode(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 295, in encode\n",
      "    return super().encode(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py\", line 266, in encode\n",
      "    return self.encode_single_device(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 375, in encode_single_device\n",
      "    inputs_batch = self.tokenizer(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3021, in __call__\n",
      "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3109, in _call_one\n",
      "    return self.batch_encode_plus(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3311, in batch_encode_plus\n",
      "    return self._batch_encode_plus(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py\", line 529, in _batch_encode_plus\n",
      "    encodings = self._tokenizer.encode_batch(\n",
      "TypeError: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 22235.22it/s]\n",
      "inferencing embedding for corpus (number=11)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=3)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 17478.69it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 19325.62it/s]\n",
      "inferencing embedding for corpus (number=29)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=9)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 51067.01it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 17947.39it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 17272.36it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 24643.38it/s]\n",
      "inferencing embedding for corpus (number=3)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 17665.19it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 23497.50it/s]\n",
      "inferencing embedding for corpus (number=16)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=4)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 36472.21it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 22333.89it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 22357.70it/s]\n",
      "inferencing embedding for corpus (number=20)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=5)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 20119.78it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 168, in find_knn_neg\n",
      "    q_vecs = model.encode_queries(queries)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 211, in encode_queries\n",
      "    return super().encode_queries(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py\", line 183, in encode_queries\n",
      "    return self.encode(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 295, in encode\n",
      "    return super().encode(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py\", line 266, in encode\n",
      "    return self.encode_single_device(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 375, in encode_single_device\n",
      "    inputs_batch = self.tokenizer(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3021, in __call__\n",
      "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3109, in _call_one\n",
      "    return self.batch_encode_plus(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3311, in batch_encode_plus\n",
      "    return self._batch_encode_plus(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py\", line 529, in _batch_encode_plus\n",
      "    encodings = self._tokenizer.encode_batch(\n",
      "TypeError: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 30283.78it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|██████████████████████| 30/30 [00:00<00:00, 6615.27it/s]\n",
      "inferencing embedding for corpus (number=12)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=3)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 25090.55it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 56730.89it/s]\n",
      "inferencing embedding for corpus (number=19)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=5)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 25461.17it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 27443.65it/s]\n",
      "inferencing embedding for corpus (number=12)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=3)--------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 168, in find_knn_neg\n",
      "    q_vecs = model.encode_queries(queries)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 211, in encode_queries\n",
      "    return super().encode_queries(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py\", line 183, in encode_queries\n",
      "    return self.encode(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 295, in encode\n",
      "    return super().encode(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py\", line 266, in encode\n",
      "    return self.encode_single_device(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 375, in encode_single_device\n",
      "    inputs_batch = self.tokenizer(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3021, in __call__\n",
      "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3109, in _call_one\n",
      "    return self.batch_encode_plus(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3311, in batch_encode_plus\n",
      "    return self._batch_encode_plus(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py\", line 529, in _batch_encode_plus\n",
      "    encodings = self._tokenizer.encode_batch(\n",
      "TypeError: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 27721.77it/s]\n",
      "inferencing embedding for corpus (number=12)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=3)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 19213.49it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 22294.32it/s]\n",
      "inferencing embedding for corpus (number=3)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 16545.58it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 22948.95it/s]\n",
      "inferencing embedding for corpus (number=22)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=5)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 25774.09it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 22001.94it/s]\n",
      "inferencing embedding for corpus (number=3)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 168, in find_knn_neg\n",
      "    q_vecs = model.encode_queries(queries)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 211, in encode_queries\n",
      "    return super().encode_queries(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py\", line 183, in encode_queries\n",
      "    return self.encode(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 295, in encode\n",
      "    return super().encode(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py\", line 266, in encode\n",
      "    return self.encode_single_device(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py\", line 375, in encode_single_device\n",
      "    inputs_batch = self.tokenizer(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3021, in __call__\n",
      "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3109, in _call_one\n",
      "    return self.batch_encode_plus(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3311, in batch_encode_plus\n",
      "    return self._batch_encode_plus(\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py\", line 529, in _batch_encode_plus\n",
      "    encodings = self._tokenizer.encode_batch(\n",
      "TypeError: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 14871.66it/s]\n",
      "inferencing embedding for corpus (number=20)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=5)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 35930.65it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 14604.12it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 14928.12it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 25837.60it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 13862.41it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 17670.15it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 21527.65it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 25080.55it/s]\n",
      "inferencing embedding for corpus (number=20)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=5)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 21583.04it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 23634.32it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 23262.92it/s]\n",
      "inferencing embedding for corpus (number=3)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 50171.10it/s]\n",
      "inferencing embedding for corpus (number=3)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 19617.89it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 11316.59it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 21112.27it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 21538.71it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 15345.01it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 19824.98it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 28024.30it/s]\n",
      "inferencing embedding for corpus (number=19)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=5)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 15503.83it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 18347.79it/s]\n",
      "inferencing embedding for corpus (number=21)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=5)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 24653.04it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 19373.23it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 24489.90it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 31512.43it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 18707.87it/s]\n",
      "inferencing embedding for corpus (number=19)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=6)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 55455.76it/s]\n",
      "inferencing embedding for corpus (number=8)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=2)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 16420.35it/s]\n",
      "inferencing embedding for corpus (number=11)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=3)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|██████████████████████| 30/30 [00:00<00:00, 5279.40it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 22957.33it/s]\n",
      "inferencing embedding for corpus (number=21)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=5)--------------\n",
      "create index and search------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 34846.06it/s]\n",
      "inferencing embedding for corpus (number=21)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=5)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 18699.53it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 15748.33it/s]\n",
      "inferencing embedding for corpus (number=19)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=5)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 17996.16it/s]\n",
      "inferencing embedding for corpus (number=20)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=5)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 22865.55it/s]\n",
      "inferencing embedding for corpus (number=20)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=5)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 20553.60it/s]\n",
      "inferencing embedding for corpus (number=20)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=5)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 21758.45it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 34913.74it/s]\n",
      "inferencing embedding for corpus (number=15)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=5)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 17398.94it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 12475.62it/s]\n",
      "inferencing embedding for corpus (number=5)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 24946.30it/s]\n",
      "inferencing embedding for corpus (number=3)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 21743.41it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 24156.10it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 24813.47it/s]\n",
      "inferencing embedding for corpus (number=3)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 14670.53it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 23917.34it/s]\n",
      "inferencing embedding for corpus (number=4)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=1)--------------\n",
      "create index and search------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 246, in <module>\n",
      "    main(data_args, model_args)\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 227, in main\n",
      "    find_knn_neg(\n",
      "  File \"/home/murad/Documents/self-study/contextual_embeddings/src/utils/../../FlagEmbedding/scripts/hn_mine.py\", line 197, in find_knn_neg\n",
      "    samples = random.sample(corpus, negative_number - len(data['neg']) + len(data['pos']))\n",
      "  File \"/home/murad/anaconda3/envs/rag/lib/python3.10/random.py\", line 482, in sample\n",
      "    raise ValueError(\"Sample larger than population or is negative\")\n",
      "ValueError: Sample larger than population or is negative\n",
      "Fetching 30 files: 100%|██████████████████████| 30/30 [00:00<00:00, 5022.12it/s]\n",
      "inferencing embedding for corpus (number=19)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=5)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 31386.66it/s]\n",
      "inferencing embedding for corpus (number=20)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=5)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 16371.21it/s]\n",
      "inferencing embedding for corpus (number=19)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=5)--------------\n",
      "create index and search------------------\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 20062.04it/s]\n",
      "inferencing embedding for corpus (number=8)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inferencing embedding for queries (number=2)--------------\n",
      "create index and search------------------\n"
     ]
    }
   ],
   "source": [
    "for doc_file in os.listdir(data_path):\n",
    "    input_file_raw = os.path.join(data_path, doc_file)\n",
    "    input_file = os.path.abspath(input_file_raw)\n",
    "\n",
    "    output_file = f\"{output_DIR}/mined_HN_{doc_file}\"\n",
    "    \n",
    "    ! python ../../FlagEmbedding/scripts/hn_mine.py \\\n",
    "        --input_file {input_file} \\\n",
    "        --output_file {output_file} \\\n",
    "        --range_for_sampling 5-50 \\\n",
    "        --negative_number 10 \\\n",
    "        --embedder_name_or_path BAAI/bge-m3 \\\n",
    "        --use_gpu_for_searching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93679716",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
