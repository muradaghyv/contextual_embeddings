{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XLMRobertaModel, XLMRobertaTokenizer\n",
    "import torch\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "model = XLMRobertaModel.from_pretrained(\"xlm-roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d446f82e05c433b81944b8bfbea81a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88e2197f95064800aeb93f9f2966682f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "869aa0db3d4c482f89b2400afa1be37c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1b605a2b0854ef9b9408fe982743e84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc0135a2e7ca44d79dd55da0230c73f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"ASAN xidmət mərkəzlərinə xoş gəlmisiniz!\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in sentences:\n",
    "    # Tokenize and convert to model input format\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    \n",
    "    # Get model output\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Extract embeddings (last hidden state)\n",
    "    embeddings = outputs.last_hidden_state\n",
    "    \n",
    "    # Find token ID for the word \"bank\"\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs.input_ids[0])\n",
    "    # bank_idx = tokens.index(\"bank\")\n",
    "    \n",
    "    # # Extract the embedding for \"bank\" in this context\n",
    "    # bank_embedding = embeddings[0, bank_idx]\n",
    "    \n",
    "    # print(f\"\\nSentence: {sentence}\")\n",
    "    # print(f\"Embedding dimension: {bank_embedding.shape}\")\n",
    "    # print(f\"First 5 values of 'bank' embedding: {bank_embedding[:5].numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " '▁',\n",
       " 'ASAN',\n",
       " '▁xidmət',\n",
       " '▁mərkəzləri',\n",
       " 'nə',\n",
       " '▁xoş',\n",
       " '▁gəl',\n",
       " 'mi',\n",
       " 'siniz',\n",
       " '!',\n",
       " '</s>']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: I am a Machine Learning Engineer.\n",
      "Embedding shape: torch.Size([1, 768])\n",
      "First 5 values: [ 0.1772717  -0.26520622  0.5473463   0.25062045  0.23591822]\n",
      "\n",
      "Sentence: Mən dizaynerliklə məşğul oluram.\n",
      "Embedding shape: torch.Size([1, 768])\n",
      "First 5 values: [-0.06480627 -0.25263363  0.12214869  0.38965446 -0.24019417]\n",
      "\n",
      "Similarity between English and Azerbaijani sentences: 0.6739\n"
     ]
    }
   ],
   "source": [
    "model_name = \"bert-base-multilingual-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Example sentences in English and a low-resource language (e.g., Azerbaijani)\n",
    "sentences = [\n",
    "    \"I am a Machine Learning Engineer.\",  # English\n",
    "    \"Mən dizaynerliklə məşğul oluram.\"  # Azerbaijani\n",
    "]\n",
    "\n",
    "# Get embeddings for both sentences\n",
    "embeddings = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    # Tokenize and get model inputs\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    \n",
    "    # Forward pass, no gradient needed\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Use the [CLS] token embedding as sentence representation\n",
    "    sentence_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "    embeddings.append(sentence_embedding)\n",
    "    \n",
    "    print(f\"Sentence: {sentence}\")\n",
    "    print(f\"Embedding shape: {sentence_embedding.shape}\")\n",
    "    print(f\"First 5 values: {sentence_embedding[0, :5].numpy()}\\n\")\n",
    "\n",
    "# Calculate similarity between the English and Azerbaijani sentences\n",
    "similarity = torch.nn.functional.cosine_similarity(embeddings[0], embeddings[1])\n",
    "print(f\"Similarity between English and Azerbaijani sentences: {similarity.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/murad/anaconda3/envs/nlp/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'cached_download' (from 'huggingface_hub.file_download') is deprecated and will be removed from version '0.26'. Use `hf_hub_download` instead.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a575cadd1d9a4646b078c1cad9b43cf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes:   0%|          | 0.00/1.23k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dadcb08d2d9643c78784f594e3929700",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d61d21d21c21466e88ecf63c4f68f38d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ac8eedfb29247e3b18b7587d880cc34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa2160ac521a43a7a0b9374bb7e9e284",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ffa75fb83e644bba55a6875c4095cd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data_config.json:   0%|          | 0.00/39.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07d1953ef7d948a288f7e8a0deb0333f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc07e9da03f94d7887bd197ecbebe9a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.onnx:   0%|          | 0.00/90.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e8a0f51df514214b1b058a4dfec5e41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_O1.onnx:   0%|          | 0.00/90.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9109e9d0e724059a94a2e2dd4536a1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_O2.onnx:   0%|          | 0.00/90.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbbed26897f64e4e9119bbe8e54206ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_O3.onnx:   0%|          | 0.00/90.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9213d6df2ea542469199d53eb376d96c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_O4.onnx:   0%|          | 0.00/45.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cf330c762b0405e977f1e360cf5a1e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_qint8_arm64.onnx:   0%|          | 0.00/23.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d0d9c4b0da84dfeb7eddb05239ada55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_qint8_avx512.onnx:   0%|          | 0.00/23.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e345d0cf03dc4a9091ac7c669ac56538",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_qint8_avx512_vnni.onnx:   0%|          | 0.00/23.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "320fb183e3254c6bbd82c7c796c22db0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_quint8_avx2.onnx:   0%|          | 0.00/23.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11bcb261a15644be9b9d4f44c4587300",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "openvino_model.bin:   0%|          | 0.00/90.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcd008758cab4abf9f3f84105261c6c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "openvino_model.xml:   0%|          | 0.00/211k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d74b6bb3f6cd45b89d4f8902b86ac770",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "openvino_model_qint8_quantized.bin:   0%|          | 0.00/22.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9f01ab5c32d4981b6c0b499994a31a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "openvino_model_qint8_quantized.xml:   0%|          | 0.00/368k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b334bf9a1b44f2a9c30ba31aae4a040",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad28b493e2ec4d0eac967f68dc9df577",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baa12d4c5bca49318cf0ef981ae6dc64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd9a2d92e08445c2bdb0762318c156cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a512bae63efc4ea5b0d149d7259b4fd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6acc831d0474b23ac986528477d3a0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train_script.py:   0%|          | 0.00/13.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "300c2a66c3954a5ba8567017f6eeab3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e98b80901874d9486f00746b3d25e10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/murad/anaconda3/envs/nlp/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What are contextual embeddings in NLP?\n",
      "\n",
      "Retrieved relevant documents:\n",
      "1. Contextual embeddings capture word meaning based on surrounding context. (similarity: 0.7027)\n",
      "2. BERT is a transformer-based model that generates contextual embeddings. (similarity: 0.6570)\n"
     ]
    }
   ],
   "source": [
    "# 1. Load embedding model (better to use sentence-transformers for this)\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# 2. Example knowledge base (in production, this would be much larger)\n",
    "documents = [\n",
    "    \"Python is a high-level programming language known for its readability.\",\n",
    "    \"TensorFlow is a machine learning framework developed by Google.\",\n",
    "    \"PyTorch is a machine learning framework developed by Facebook.\",\n",
    "    \"Contextual embeddings capture word meaning based on surrounding context.\",\n",
    "    \"BERT is a transformer-based model that generates contextual embeddings.\"\n",
    "]\n",
    "\n",
    "# 3. Encode documents (create embeddings)\n",
    "document_embeddings = model.encode(documents)\n",
    "\n",
    "# 4. RAG retrieval function\n",
    "def retrieve_relevant_context(query, top_k=2):\n",
    "    # Encode the query\n",
    "    query_embedding = model.encode([query])[0]\n",
    "    \n",
    "    # Calculate similarity with all documents\n",
    "    similarities = cosine_similarity([query_embedding], document_embeddings)[0]\n",
    "    \n",
    "    # Get top-k most similar documents\n",
    "    top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "    \n",
    "    # Return relevant documents and their similarity scores\n",
    "    results = []\n",
    "    for idx in top_indices:\n",
    "        results.append({\n",
    "            \"document\": documents[idx],\n",
    "            \"similarity\": similarities[idx]\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 5. Example query\n",
    "query = \"What are contextual embeddings in NLP?\"\n",
    "relevant_docs = retrieve_relevant_context(query)\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(\"Retrieved relevant documents:\")\n",
    "for i, doc in enumerate(relevant_docs):\n",
    "    print(f\"{i+1}. {doc['document']} (similarity: {doc['similarity']:.4f})\")\n",
    "\n",
    "# In a complete RAG system, these retrieved documents would be passed to \n",
    "# a generative model (like GPT) to produce the final response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tokens: ['the', 'cat', 'sat', 'on', 'the', 'mat', '.']\n",
      "Masked tokens: ['the', '[unused794]', 'sat', 'on', 'the', 'mat', '.']\n",
      "MLM Loss: 2.898212432861328\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Example sentence\n",
    "text = \"The cat sat on the mat.\"\n",
    "\n",
    "# Tokenize\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(f\"Original tokens: {tokens}\")\n",
    "\n",
    "# Create input IDs\n",
    "input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "# Create a copy of input IDs for the labels (ground truth)\n",
    "labels = input_ids.copy()\n",
    "\n",
    "# Constants for masking procedure\n",
    "mask_token_id = tokenizer.convert_tokens_to_ids(['[MASK]'])[0]\n",
    "vocab_size = tokenizer.vocab_size\n",
    "masking_prob = 0.15\n",
    "\n",
    "# Randomly mask tokens for MLM\n",
    "for i in range(len(input_ids)):\n",
    "    if random.random() < masking_prob:  # 15% chance to mask\n",
    "        rand = random.random()\n",
    "        \n",
    "        if rand < 0.8:  # 80% of the time, replace with [MASK]\n",
    "            input_ids[i] = mask_token_id\n",
    "        elif rand < 0.9:  # 10% of the time, replace with random word\n",
    "            input_ids[i] = random.randint(0, vocab_size - 1)\n",
    "        # 10% of the time, keep the word unchanged\n",
    "    \n",
    "# Convert to tensor format for the model\n",
    "input_tensor = torch.tensor([input_ids])\n",
    "labels_tensor = torch.tensor([labels])\n",
    "\n",
    "# In real training, tokens where no masking occurred would have label = -100\n",
    "# to ignore them in the loss calculation\n",
    "\n",
    "# Forward pass and calculate loss\n",
    "outputs = model(input_tensor, labels=labels_tensor)\n",
    "loss = outputs.loss\n",
    "\n",
    "print(f\"Masked tokens: {tokenizer.convert_ids_to_tokens(input_ids)}\")\n",
    "print(f\"MLM Loss: {loss.item()}\")\n",
    "\n",
    "# During actual pre-training, this loss would be backpropagated to update the model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classify these Azerbaijani sentences as positive or negative.\n",
      "\n",
      "Sentence: Bu film çox maraqlı idi. (This movie was very interesting.)\n",
      "Sentiment: Positive\n",
      "\n",
      "Sentence: Hava bu gün çox gözəldir. (The weather is very beautiful today.)\n",
      "Sentiment: Positive\n",
      "\n",
      "Sentence: Yeməkdən razı deyiləm. (I am not satisfied with the food.)\n",
      "Sentiment: Negative\n",
      "\n",
      "Sentence: Kitab məni məyus etdi. (The book disappointed me.)\n",
      "Sentiment: Negative\n",
      "\n",
      "Sentence: Bu məhsul keyfiyyətli deyil. (This product is not of good quality.)\n",
      "Sentiment: \n",
      "\n",
      "Sentence: Yeməkd\n"
     ]
    }
   ],
   "source": [
    "# Load model and tokenizer (using a smaller model for example purposes)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Few-shot examples for sentiment classification in a low-resource language (Azerbaijani)\n",
    "few_shot_prompt = \"\"\"\n",
    "Classify these Azerbaijani sentences as positive or negative.\n",
    "\n",
    "Sentence: Bu film çox maraqlı idi. (This movie was very interesting.)\n",
    "Sentiment: Positive\n",
    "\n",
    "Sentence: Hava bu gün çox gözəldir. (The weather is very beautiful today.)\n",
    "Sentiment: Positive\n",
    "\n",
    "Sentence: Yeməkdən razı deyiləm. (I am not satisfied with the food.)\n",
    "Sentiment: Negative\n",
    "\n",
    "Sentence: Kitab məni məyus etdi. (The book disappointed me.)\n",
    "Sentiment: Negative\n",
    "\n",
    "Sentence: Bu məhsul keyfiyyətli deyil. (This product is not of good quality.)\n",
    "Sentiment: \n",
    "\"\"\"\n",
    "\n",
    "# Tokenize and generate\n",
    "inputs = tokenizer(few_shot_prompt, return_tensors=\"pt\")\n",
    "outputs = model.generate(\n",
    "    inputs.input_ids,\n",
    "    max_length=len(inputs.input_ids[0]) + 10,\n",
    "    temperature=0.7,\n",
    "    num_return_sequences=1,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "# Decode and print result\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized sentence: ['[CLS]', 'M', '##ən', 't', '##əbii', 'dil', 'em', '##alı', '##nı', 'se', '##vir', '##əm', '.', '[SEP]']\n",
      "Sentence embedding shape: torch.Size([1, 768])\n",
      "Token embeddings shape: torch.Size([14, 768])\n",
      "Token: [CLS], Embedding shape: torch.Size([768])\n",
      "First 5 values: [-0.051672    0.07287467  0.20710555  0.45764723 -0.06344569]\n",
      "Token: M, Embedding shape: torch.Size([768])\n",
      "First 5 values: [ 0.42265168  0.06584752  0.8201619   0.48402944 -0.25524417]\n",
      "Token: ##ən, Embedding shape: torch.Size([768])\n",
      "First 5 values: [ 0.03612332  0.1700448   0.92322004  0.45827347 -0.39794305]\n",
      "Token: t, Embedding shape: torch.Size([768])\n",
      "First 5 values: [ 0.15537417 -0.37486845  1.3681626   0.67887956 -0.6675606 ]\n",
      "Token: ##əbii, Embedding shape: torch.Size([768])\n",
      "First 5 values: [ 0.1742309  -0.31118524  1.2627839   0.96189666 -0.39453092]\n",
      "Token: dil, Embedding shape: torch.Size([768])\n",
      "First 5 values: [ 0.22533163  0.16853754  1.3803712   0.49217775 -0.3043777 ]\n",
      "Token: em, Embedding shape: torch.Size([768])\n",
      "First 5 values: [ 0.1664819   0.03111231  1.2374514   0.196477   -0.34285906]\n",
      "Token: ##alı, Embedding shape: torch.Size([768])\n",
      "First 5 values: [0.24369772 0.0321746  0.9091978  0.8007826  0.7440479 ]\n",
      "Token: ##nı, Embedding shape: torch.Size([768])\n",
      "First 5 values: [-0.2780866  -0.6196055  -0.31524345  1.1620973  -0.27340665]\n",
      "Token: se, Embedding shape: torch.Size([768])\n",
      "First 5 values: [ 0.5865598   0.00457839 -0.09219665  0.3451133  -0.2335676 ]\n",
      "Token: ##vir, Embedding shape: torch.Size([768])\n",
      "First 5 values: [0.37929547 0.34064135 0.50444984 0.81982523 0.05155078]\n",
      "Token: ##əm, Embedding shape: torch.Size([768])\n",
      "First 5 values: [ 0.00211693 -0.23030087  0.28199247  0.550155    0.01586519]\n",
      "Token: ., Embedding shape: torch.Size([768])\n",
      "First 5 values: [ 0.03679214 -0.15948607  0.4297805   0.8414165   0.10769602]\n",
      "Token: [SEP], Embedding shape: torch.Size([768])\n",
      "First 5 values: [ 0.03127975 -0.06742389  0.8438172   0.5405642  -0.21958269]\n"
     ]
    }
   ],
   "source": [
    "# Load multilingual BERT\n",
    "model_name = \"bert-base-multilingual-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Example sentence in Azerbaijani\n",
    "sentence = \"Mən təbii dil emalını sevirəm.\"  # \"I love natural language processing.\"\n",
    "\n",
    "# Tokenize\n",
    "inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs.input_ids[0])\n",
    "\n",
    "print(f\"Tokenized sentence: {tokens}\")\n",
    "\n",
    "# Get embeddings\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Get sentence-level embedding (using [CLS] token)\n",
    "sentence_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "print(f\"Sentence embedding shape: {sentence_embedding.shape}\")\n",
    "\n",
    "# Get token-level embeddings\n",
    "token_embeddings = outputs.last_hidden_state[0]\n",
    "print(f\"Token embeddings shape: {token_embeddings.shape}\")\n",
    "\n",
    "# Print embedding for each token\n",
    "for i, token in enumerate(tokens):\n",
    "    print(f\"Token: {token}, Embedding shape: {token_embeddings[i].shape}\")\n",
    "    print(f\"First 5 values: {token_embeddings[i][:5].numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
